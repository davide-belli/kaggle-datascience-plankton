{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davide/miniconda3/envs/honours/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Import libraries for doing image analysis\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "import glob\n",
    "import os\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import StratifiedKFold as KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from pylab import cm\n",
    "from skimage import segmentation\n",
    "from skimage.morphology import watershed\n",
    "from skimage import measure\n",
    "from skimage import morphology\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import ndimage\n",
    "from skimage.feature import peak_local_max\n",
    "import tensorflow as tf\n",
    "from math import ceil, floor\n",
    "import time\n",
    "\n",
    "# make graphics inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the classnames from the directory structure\n",
    "directory_names = list(set(glob.glob(os.path.join(\"competition_data\",\"train\", \"*\"))\\\n",
    " ).difference(set(glob.glob(os.path.join(\"competition_data\",\"train\",\"*.*\")))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading images\n",
      "10.0 % done\n",
      "20.0 % done\n",
      "30.0 % done\n",
      "40.0 % done\n",
      "50.0 % done\n",
      "60.0 % done\n",
      "70.0 % done\n",
      "80.0 % done\n",
      "90.0 % done\n",
      "100.0 % done\n"
     ]
    }
   ],
   "source": [
    "# Rescale the images and create the combined metrics and training labels\n",
    "\n",
    "#get the total training images\n",
    "numberofImages = 0\n",
    "for folder in directory_names:\n",
    "    for fileNameDir in os.walk(folder):   \n",
    "        for fileName in fileNameDir[2]:\n",
    "             # Only read in the images\n",
    "            if fileName[-4:] != \".jpg\":\n",
    "              continue\n",
    "            numberofImages += 1\n",
    "            \n",
    "# We'll rescale the images to be 25x25\n",
    "maxPixel = 28\n",
    "imageSize = maxPixel * maxPixel\n",
    "num_rows = numberofImages # one row for each image in the training dataset\n",
    "num_features = imageSize # + 1 # for our ratio\n",
    "\n",
    "# X is the feature vector with one row of features per image\n",
    "# consisting of the pixel values and our metric\n",
    "X = np.zeros((num_rows, num_features), dtype=float)\n",
    "# y is the numeric class label \n",
    "y = np.zeros((num_rows))\n",
    "\n",
    "files = []\n",
    "# Generate training data\n",
    "i = 0    \n",
    "label = 0\n",
    "# List of string of class names\n",
    "namesClasses = list()\n",
    "\n",
    "print(\"Reading images\")\n",
    "# Navigate through the list of directories\n",
    "for folder in directory_names:\n",
    "    # Append the string class name for each class\n",
    "    currentClass = folder.split(os.pathsep)[-1]\n",
    "    namesClasses.append(currentClass)\n",
    "    for fileNameDir in os.walk(folder):   \n",
    "        for fileName in fileNameDir[2]:\n",
    "            # Only read in the images\n",
    "            if fileName[-4:] != \".jpg\":\n",
    "              continue\n",
    "\n",
    "            # Read in the images and create the features\n",
    "            nameFileImage = \"{0}{1}{2}\".format(fileNameDir[0], os.sep, fileName)            \n",
    "            image = imread(nameFileImage, as_grey=True)\n",
    "            files.append(nameFileImage)\n",
    "            #axisratio = getMinorMajorRatio(image)\n",
    "            image = resize(image, (maxPixel, maxPixel))\n",
    "\n",
    "            # Store the rescaled image pixels and the axis ratio\n",
    "            X[i, 0:imageSize] = np.reshape(image, (1, imageSize))\n",
    "            #X[i, imageSize] = axisratio\n",
    "\n",
    "            # Store the classlabel\n",
    "            y[i] = label\n",
    "            i += 1\n",
    "            # report progress for each 5% done  \n",
    "            report = [int((j+1)*num_rows/10.) for j in range(10)]\n",
    "            if i in report: \n",
    "                print (np.ceil(i *100.0 / num_rows), \"% done\")\n",
    "    label += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = maxPixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flip_images(X_imgs):\n",
    "    X_flip = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape = (IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "    tf_img1 = tf.image.flip_left_right(X)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for img in X_imgs:\n",
    "            flipped_imgs = sess.run([tf_img1], feed_dict = {X: img})\n",
    "            X_flip.extend(flipped_imgs)\n",
    "    X_flip = np.array(X_flip, dtype = np.float32)\n",
    "    return np.concatenate((X_imgs, X_flip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rotate_images(X_imgs):\n",
    "    X_rotate = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape = (IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "    k = tf.placeholder(tf.int32)\n",
    "    tf_img = tf.image.rot90(X, k = k)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for img in X_imgs:\n",
    "            for i in range(3):  # Rotation at 90, 180 and 270 degrees\n",
    "                rotated_img = sess.run(tf_img, feed_dict = {X: img, k: i + 1})\n",
    "                X_rotate.append(rotated_img)\n",
    "        \n",
    "    X_rotate = np.array(X_rotate, dtype = np.float32)\n",
    "    return np.concatenate((X_imgs, X_rotate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_images(X_imgs, scales=[0.80, 0.90]):\n",
    "    # Various settings needed for Tensorflow operation\n",
    "    boxes = np.zeros((len(scales), 4), dtype = np.float32)\n",
    "    for index, scale in enumerate(scales):\n",
    "        x1 = y1 = 0.5 - 0.5 * scale # To scale centrally\n",
    "        x2 = y2 = 0.5 + 0.5 * scale\n",
    "        boxes[index] = np.array([y1, x1, y2, x2], dtype = np.float32)\n",
    "    box_ind = np.zeros((len(scales)), dtype = np.int32)\n",
    "    crop_size = np.array([maxPixel, maxPixel], dtype = np.int32)\n",
    "    \n",
    "    X_scale_data = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape = (1, maxPixel, maxPixel, 1))\n",
    "    # Define Tensorflow operation for all scales but only one base image at a time\n",
    "    tf_img = tf.image.crop_and_resize(X, boxes, box_ind, crop_size)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for img_data in X_imgs:\n",
    "            batch_img = np.expand_dims(img_data, axis = 0)\n",
    "            scaled_imgs = sess.run(tf_img, feed_dict = {X: batch_img})\n",
    "            X_scale_data.extend(scaled_imgs)\n",
    "    \n",
    "    X_scale_data = np.array(X_scale_data, dtype = np.float32)\n",
    "    return np.concatenate((X_imgs, X_scale_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_translate_parameters(index):\n",
    "    if index == 0: # Translate left 20 percent\n",
    "        offset = np.array([0.0, 0.2], dtype = np.float32)\n",
    "        size = np.array([IMAGE_SIZE, ceil(0.8 * IMAGE_SIZE)], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = int(ceil(0.8 * IMAGE_SIZE))\n",
    "        h_start = 0\n",
    "        h_end = IMAGE_SIZE\n",
    "    elif index == 1: # Translate right 20 percent\n",
    "        offset = np.array([0.0, -0.2], dtype = np.float32)\n",
    "        size = np.array([IMAGE_SIZE, ceil(0.8 * IMAGE_SIZE)], dtype = np.int32)\n",
    "        w_start = int(floor((1 - 0.8) * IMAGE_SIZE))\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = 0\n",
    "        h_end = IMAGE_SIZE\n",
    "    elif index == 2: # Translate top 20 percent\n",
    "        offset = np.array([0.2, 0.0], dtype = np.float32)\n",
    "        size = np.array([ceil(0.8 * IMAGE_SIZE), IMAGE_SIZE], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = 0\n",
    "        h_end = int(ceil(0.8 * IMAGE_SIZE)) \n",
    "    else: # Translate bottom 20 percent\n",
    "        offset = np.array([-0.2, 0.0], dtype = np.float32)\n",
    "        size = np.array([ceil(0.8 * IMAGE_SIZE), IMAGE_SIZE], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = int(floor((1 - 0.8) * IMAGE_SIZE))\n",
    "        h_end = IMAGE_SIZE \n",
    "        \n",
    "    return offset, size, w_start, w_end, h_start, h_end\n",
    "\n",
    "def translate_images(X_imgs):\n",
    "    offsets = np.zeros((len(X_imgs), 2), dtype = np.float32)\n",
    "    n_translations = 4\n",
    "    X_translated_arr = []\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(n_translations):\n",
    "            X_translated = np.zeros((len(X_imgs), IMAGE_SIZE, IMAGE_SIZE, 1), \n",
    "\t\t\t\t    dtype = np.float32)\n",
    "            X_translated.fill(1.0) # Filling background color\n",
    "            base_offset, size, w_start, w_end, h_start, h_end = get_translate_parameters(i)\n",
    "            offsets[:, :] = base_offset \n",
    "            glimpses = tf.image.extract_glimpse(X_imgs, size, offsets)\n",
    "            \n",
    "            glimpses = sess.run(glimpses)\n",
    "            X_translated[:, h_start: h_start + size[0], \\\n",
    "\t\t\t w_start: w_start + size[1], :] = glimpses\n",
    "            X_translated_arr.extend(X_translated)\n",
    "    X_translated_arr = np.array(X_translated_arr, dtype = np.float32)\n",
    "    return np.concatenate((X_imgs, X_translated_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment_dataset(X_imgs, y_imgs):\n",
    "    assert len(X_imgs) == len(y_imgs)\n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "    i = 0\n",
    "    print(\"Starting Dataset Augmentation...\")\n",
    "    \n",
    "    for i in range(len(y_imgs)):\n",
    "        \n",
    "        imgs = [np.reshape(X_imgs[i,:],(IMAGE_SIZE,IMAGE_SIZE,1))]\n",
    "        label = [y_imgs[i]]\n",
    "        \n",
    "        imgs = flip_images(imgs)\n",
    "        imgs = rotate_images(imgs)\n",
    "        imgs = scale_images(imgs)\n",
    "        imgs = translate_images(imgs)\n",
    "        \n",
    "        labels = label * len(imgs)\n",
    "#         print(len(labels), imgs.shape)\n",
    "        \n",
    "        X_aug += [np.array(x) for x in imgs.tolist()[:]]\n",
    "        y_aug += labels\n",
    "        \n",
    "        i += 1\n",
    "        # report progress for each 5% done  \n",
    "        report = [int((j+1)*len(y_imgs)/10.) for j in range(10)]\n",
    "        if i in report: \n",
    "            print (np.ceil(i *100.0 / len(y_imgs)), \"% done\")\n",
    "        \n",
    "    return X_aug, y_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    }
   ],
   "source": [
    "N_CLASSES = len(set(y))\n",
    "print(N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "    # Plankton images are 25x25 pixels, and have one color channel\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "    # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2\n",
    "    # Computes 64 features using a 5x5 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "    # Dense Layer\n",
    "    # Densely connected layer with 1024 neurons\n",
    "    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    # Output Tensor Shape: [batch_size, 1024]\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "    dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits layer\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    logits = tf.layers.dense(inputs=dropout, units=121) # N_CLASSES\n",
    "\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train / Total:  24269 / 30336\n",
      "Shapes: (24269, 784) (24269,) (6067, 784) (6067,)\n"
     ]
    }
   ],
   "source": [
    "#Generate indexes to shuffle dataset and create testset\n",
    "\n",
    "idx = np.arange(len(y))\n",
    "np.random.seed(seed=1234)\n",
    "np.random.shuffle(idx)\n",
    "separator = ceil(len(y)/10*8)\n",
    "print(\"Train / Total: \", separator, \"/\", len(y))\n",
    "\n",
    "train_data = X[idx[:separator]].astype('float32')\n",
    "eval_data = X[idx[separator:]].astype('float32')\n",
    "train_labels = y[idx[:separator]].astype(int)\n",
    "eval_labels = y[idx[separator:]].astype(int)\n",
    "\n",
    "train_data = train_data #[:, 0:-1] # Returns np.array, remove axis rateo feature\n",
    "eval_data = eval_data #[:, 0:-1]\n",
    "\n",
    "print(\"Shapes:\", train_data.shape, train_labels.shape, eval_data.shape, eval_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(train_data, train_labels, eval_data, eval_labels):\n",
    "    # Create the Estimator\n",
    "    now = str(time.time())[-5:]\n",
    "    image_classifier = tf.estimator.Estimator(\n",
    "      model_fn=cnn_model_fn, model_dir=\"/tmp/convnet_model_\"+now)\n",
    "\n",
    "    # Set up logging for predictions\n",
    "    # Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
    "    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "    # Train the model\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": train_data},\n",
    "      y=train_labels,\n",
    "      batch_size=100,\n",
    "      num_epochs=None,\n",
    "      shuffle=True)\n",
    "    image_classifier.train(\n",
    "      input_fn=train_input_fn,\n",
    "      steps=10000)#,\n",
    "      #hooks=[logging_hook])\n",
    "\n",
    "    # Evaluate the model and print results\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": eval_data},\n",
    "      y=eval_labels,\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "    eval_results = image_classifier.evaluate(input_fn=eval_input_fn)\n",
    "#     print(eval_results)\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpsq4eboha\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpsq4eboha', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f85d0392e10>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpsq4eboha/model.ckpt.\n",
      "INFO:tensorflow:loss = 4.79103, step = 1\n",
      "INFO:tensorflow:global_step/sec: 47.2715\n",
      "INFO:tensorflow:loss = 4.74089, step = 101 (2.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.0223\n",
      "INFO:tensorflow:loss = 4.73784, step = 201 (1.922 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1211\n",
      "INFO:tensorflow:loss = 4.55918, step = 301 (1.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.0726\n",
      "INFO:tensorflow:loss = 4.39403, step = 401 (1.921 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1331\n",
      "INFO:tensorflow:loss = 4.38558, step = 501 (1.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.8926\n",
      "INFO:tensorflow:loss = 4.25256, step = 601 (1.927 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.6314\n",
      "INFO:tensorflow:loss = 4.182, step = 701 (1.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2132\n",
      "INFO:tensorflow:loss = 4.31732, step = 801 (1.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.0757\n",
      "INFO:tensorflow:loss = 4.3265, step = 901 (1.921 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.132\n",
      "INFO:tensorflow:loss = 4.32928, step = 1001 (1.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2384\n",
      "INFO:tensorflow:loss = 4.11228, step = 1101 (1.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2271\n",
      "INFO:tensorflow:loss = 4.22446, step = 1201 (1.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.9537\n",
      "INFO:tensorflow:loss = 4.26238, step = 1301 (1.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.8308\n",
      "INFO:tensorflow:loss = 4.03011, step = 1401 (2.008 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.43\n",
      "INFO:tensorflow:loss = 4.10614, step = 1501 (2.063 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.409\n",
      "INFO:tensorflow:loss = 4.18776, step = 1601 (2.024 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.0155\n",
      "INFO:tensorflow:loss = 4.28859, step = 1701 (2.000 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.3676\n",
      "INFO:tensorflow:loss = 4.19223, step = 1801 (1.985 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.2199\n",
      "INFO:tensorflow:loss = 4.32382, step = 1901 (1.952 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.025\n",
      "INFO:tensorflow:loss = 4.14518, step = 2001 (1.922 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.6529\n",
      "INFO:tensorflow:loss = 4.19164, step = 2101 (1.975 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.9568\n",
      "INFO:tensorflow:loss = 4.3909, step = 2201 (1.962 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.3144\n",
      "INFO:tensorflow:loss = 4.10302, step = 2301 (1.988 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.6042\n",
      "INFO:tensorflow:loss = 4.28312, step = 2401 (1.939 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.4462\n",
      "INFO:tensorflow:loss = 4.18363, step = 2501 (1.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.6831\n",
      "INFO:tensorflow:loss = 4.00288, step = 2601 (1.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.9261\n",
      "INFO:tensorflow:loss = 4.28762, step = 2701 (1.963 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.2206\n",
      "INFO:tensorflow:loss = 4.11813, step = 2801 (2.033 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.4492\n",
      "INFO:tensorflow:loss = 4.23345, step = 2901 (1.981 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.7201\n",
      "INFO:tensorflow:loss = 4.28147, step = 3001 (1.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.5815\n",
      "INFO:tensorflow:loss = 4.32661, step = 3101 (1.977 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.1224\n",
      "INFO:tensorflow:loss = 4.10015, step = 3201 (1.995 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.4059\n",
      "INFO:tensorflow:loss = 4.15909, step = 3301 (2.066 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.2699\n",
      "INFO:tensorflow:loss = 4.14284, step = 3401 (1.989 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.9226\n",
      "INFO:tensorflow:loss = 4.08484, step = 3501 (2.003 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.6647\n",
      "INFO:tensorflow:loss = 3.96973, step = 3601 (1.974 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.953\n",
      "INFO:tensorflow:loss = 4.07834, step = 3701 (1.924 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.3962\n",
      "INFO:tensorflow:loss = 4.03522, step = 3801 (1.986 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.4748\n",
      "INFO:tensorflow:loss = 3.97292, step = 3901 (2.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.3086\n",
      "INFO:tensorflow:loss = 4.03678, step = 4001 (1.988 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2152\n",
      "INFO:tensorflow:loss = 4.08242, step = 4101 (1.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.005\n",
      "INFO:tensorflow:loss = 4.13221, step = 4201 (1.923 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.9728\n",
      "INFO:tensorflow:loss = 3.99231, step = 4301 (1.924 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.9985\n",
      "INFO:tensorflow:loss = 4.00386, step = 4401 (1.923 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.187\n",
      "INFO:tensorflow:loss = 3.69225, step = 4501 (1.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1472\n",
      "INFO:tensorflow:loss = 4.01279, step = 4601 (1.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.0349\n",
      "INFO:tensorflow:loss = 3.98155, step = 4701 (1.922 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.0642\n",
      "INFO:tensorflow:loss = 4.14214, step = 4801 (1.921 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1136\n",
      "INFO:tensorflow:loss = 4.02935, step = 4901 (1.919 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1604\n",
      "INFO:tensorflow:loss = 3.89746, step = 5001 (1.917 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1969\n",
      "INFO:tensorflow:loss = 3.96475, step = 5101 (1.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1065\n",
      "INFO:tensorflow:loss = 4.07329, step = 5201 (1.919 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2535\n",
      "INFO:tensorflow:loss = 4.12429, step = 5301 (1.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2226\n",
      "INFO:tensorflow:loss = 3.94302, step = 5401 (1.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2295\n",
      "INFO:tensorflow:loss = 3.82228, step = 5501 (1.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4272\n",
      "INFO:tensorflow:loss = 3.84984, step = 5601 (1.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2399\n",
      "INFO:tensorflow:loss = 3.95949, step = 5701 (1.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3435\n",
      "INFO:tensorflow:loss = 3.90275, step = 5801 (1.910 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2299\n",
      "INFO:tensorflow:loss = 3.90711, step = 5901 (1.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3331\n",
      "INFO:tensorflow:loss = 3.99601, step = 6001 (1.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1824\n",
      "INFO:tensorflow:loss = 3.84928, step = 6101 (1.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2129\n",
      "INFO:tensorflow:loss = 3.99288, step = 6201 (1.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1047\n",
      "INFO:tensorflow:loss = 3.99563, step = 6301 (1.919 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2408\n",
      "INFO:tensorflow:loss = 3.84127, step = 6401 (1.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2607\n",
      "INFO:tensorflow:loss = 3.73705, step = 6501 (1.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1494\n",
      "INFO:tensorflow:loss = 4.01829, step = 6601 (1.917 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2815\n",
      "INFO:tensorflow:loss = 3.83703, step = 6701 (1.913 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.4725\n",
      "INFO:tensorflow:loss = 3.83919, step = 6801 (1.943 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.105\n",
      "INFO:tensorflow:loss = 3.80391, step = 6901 (1.919 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3748\n",
      "INFO:tensorflow:loss = 4.03772, step = 7001 (1.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3355\n",
      "INFO:tensorflow:loss = 3.8919, step = 7101 (1.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3525\n",
      "INFO:tensorflow:loss = 3.73196, step = 7201 (1.910 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3193\n",
      "INFO:tensorflow:loss = 3.82403, step = 7301 (1.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.9489\n",
      "INFO:tensorflow:loss = 3.94913, step = 7401 (2.002 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.9893\n",
      "INFO:tensorflow:loss = 3.5612, step = 7501 (2.084 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.1775\n",
      "INFO:tensorflow:loss = 3.78677, step = 7601 (2.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.7155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 3.99635, step = 7701 (1.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.4329\n",
      "INFO:tensorflow:loss = 3.87016, step = 7801 (2.065 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.5061\n",
      "INFO:tensorflow:loss = 3.89188, step = 7901 (2.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.157\n",
      "INFO:tensorflow:loss = 3.69832, step = 8001 (1.992 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.367\n",
      "INFO:tensorflow:loss = 3.68083, step = 8101 (1.910 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.1084\n",
      "INFO:tensorflow:loss = 3.97801, step = 8201 (1.996 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.6437\n",
      "INFO:tensorflow:loss = 3.65734, step = 8301 (2.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.4827\n",
      "INFO:tensorflow:loss = 3.80444, step = 8401 (1.981 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1293\n",
      "INFO:tensorflow:loss = 3.73515, step = 8501 (1.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2415\n",
      "INFO:tensorflow:loss = 3.88903, step = 8601 (1.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.223\n",
      "INFO:tensorflow:loss = 3.63184, step = 8701 (1.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1982\n",
      "INFO:tensorflow:loss = 3.93873, step = 8801 (1.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2412\n",
      "INFO:tensorflow:loss = 3.75024, step = 8901 (1.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4124\n",
      "INFO:tensorflow:loss = 3.64364, step = 9001 (1.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1409\n",
      "INFO:tensorflow:loss = 3.96046, step = 9101 (1.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2428\n",
      "INFO:tensorflow:loss = 3.6991, step = 9201 (1.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1359\n",
      "INFO:tensorflow:loss = 3.65594, step = 9301 (1.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3159\n",
      "INFO:tensorflow:loss = 3.68473, step = 9401 (1.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3962\n",
      "INFO:tensorflow:loss = 3.60203, step = 9501 (1.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2954\n",
      "INFO:tensorflow:loss = 3.61731, step = 9601 (1.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1607\n",
      "INFO:tensorflow:loss = 3.95248, step = 9701 (1.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2023\n",
      "INFO:tensorflow:loss = 3.65011, step = 9801 (1.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1125\n",
      "INFO:tensorflow:loss = 3.8596, step = 9901 (1.918 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/tmpsq4eboha/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.56821.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-21-00:57:50\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpsq4eboha/model.ckpt-10000\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-21-00:57:50\n",
      "INFO:tensorflow:Saving dict for global step 10000: accuracy = 0.19433, global_step = 10000, loss = 3.56035\n"
     ]
    }
   ],
   "source": [
    "basic_results = run_experiment(train_data, train_labels, eval_data, eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Dataset Augmentation...\n",
      "10.0 % done\n",
      "20.0 % done\n",
      "30.0 % done\n",
      "40.0 % done\n",
      "50.0 % done\n",
      "60.0 % done\n",
      "70.0 % done\n",
      "80.0 % done\n",
      "90.0 % done\n",
      "100.0 % done\n",
      "Augmented dataset:  (284760, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data_AUGM, train_labels_AUGM = augment_dataset(train_data, train_labels)\n",
    "\n",
    "train_data_AUGM = np.array(train_data_AUGM, dtype='float32')\n",
    "train_labels_AUGM = np.array(train_labels_AUGM, dtype='int')\n",
    "\n",
    "print(\"Augmented dataset: \", train_data_AUGM.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/image_convnet_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8f806f0358>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/image_convnet_model/model.ckpt-48004\n",
      "INFO:tensorflow:Saving checkpoints for 48005 into /tmp/image_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.88094, step = 48005\n",
      "INFO:tensorflow:global_step/sec: 51.4643\n",
      "INFO:tensorflow:loss = 0.751765, step = 48105 (1.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5298\n",
      "INFO:tensorflow:loss = 1.66778, step = 48205 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4896\n",
      "INFO:tensorflow:loss = 0.634546, step = 48305 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5702\n",
      "INFO:tensorflow:loss = 1.70944, step = 48405 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6196\n",
      "INFO:tensorflow:loss = 1.62926, step = 48505 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7197\n",
      "INFO:tensorflow:loss = 1.25364, step = 48605 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.402\n",
      "INFO:tensorflow:loss = 1.23313, step = 48705 (1.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6306\n",
      "INFO:tensorflow:loss = 1.18166, step = 48805 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6033\n",
      "INFO:tensorflow:loss = 1.25284, step = 48905 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5694\n",
      "INFO:tensorflow:loss = 1.18679, step = 49005 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4402\n",
      "INFO:tensorflow:loss = 0.909477, step = 49105 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.449\n",
      "INFO:tensorflow:loss = 1.00726, step = 49205 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7426\n",
      "INFO:tensorflow:loss = 1.70312, step = 49305 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7079\n",
      "INFO:tensorflow:loss = 1.15865, step = 49405 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5979\n",
      "INFO:tensorflow:loss = 0.425586, step = 49505 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5552\n",
      "INFO:tensorflow:loss = 1.02166, step = 49605 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5378\n",
      "INFO:tensorflow:loss = 1.06767, step = 49705 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.9161\n",
      "INFO:tensorflow:loss = 1.19798, step = 49805 (2.006 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.8738\n",
      "INFO:tensorflow:loss = 1.15977, step = 49905 (2.086 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.4827\n",
      "INFO:tensorflow:loss = 0.797161, step = 50005 (2.062 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.6888\n",
      "INFO:tensorflow:loss = 1.29992, step = 50105 (1.935 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.4588\n",
      "INFO:tensorflow:loss = 0.787399, step = 50205 (1.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.6277\n",
      "INFO:tensorflow:loss = 1.185, step = 50305 (1.975 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5283\n",
      "INFO:tensorflow:loss = 0.842573, step = 50405 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4754\n",
      "INFO:tensorflow:loss = 0.924698, step = 50505 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5738\n",
      "INFO:tensorflow:loss = 1.06958, step = 50605 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5712\n",
      "INFO:tensorflow:loss = 1.32609, step = 50705 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5973\n",
      "INFO:tensorflow:loss = 1.57876, step = 50805 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5995\n",
      "INFO:tensorflow:loss = 1.09915, step = 50905 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6631\n",
      "INFO:tensorflow:loss = 0.609221, step = 51005 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.507\n",
      "INFO:tensorflow:loss = 1.21334, step = 51105 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6444\n",
      "INFO:tensorflow:loss = 0.858901, step = 51205 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4753\n",
      "INFO:tensorflow:loss = 0.889614, step = 51305 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4748\n",
      "INFO:tensorflow:loss = 1.15496, step = 51405 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5153\n",
      "INFO:tensorflow:loss = 0.739447, step = 51505 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5545\n",
      "INFO:tensorflow:loss = 1.00167, step = 51605 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5269\n",
      "INFO:tensorflow:loss = 0.878426, step = 51705 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5226\n",
      "INFO:tensorflow:loss = 1.05458, step = 51805 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4661\n",
      "INFO:tensorflow:loss = 1.18199, step = 51905 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.493\n",
      "INFO:tensorflow:loss = 0.758671, step = 52005 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5031\n",
      "INFO:tensorflow:loss = 1.10006, step = 52105 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5052\n",
      "INFO:tensorflow:loss = 1.6013, step = 52205 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.483\n",
      "INFO:tensorflow:loss = 1.03733, step = 52305 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4569\n",
      "INFO:tensorflow:loss = 1.01774, step = 52405 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6346\n",
      "INFO:tensorflow:loss = 1.39261, step = 52505 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4983\n",
      "INFO:tensorflow:loss = 0.82376, step = 52605 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5319\n",
      "INFO:tensorflow:loss = 0.699305, step = 52705 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5258\n",
      "INFO:tensorflow:loss = 1.20071, step = 52805 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4786\n",
      "INFO:tensorflow:loss = 0.923829, step = 52905 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7608\n",
      "INFO:tensorflow:loss = 0.923757, step = 53005 (1.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.639\n",
      "INFO:tensorflow:loss = 1.04544, step = 53105 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3674\n",
      "INFO:tensorflow:loss = 0.71289, step = 53205 (1.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4874\n",
      "INFO:tensorflow:loss = 1.23527, step = 53305 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.0128\n",
      "INFO:tensorflow:loss = 0.931275, step = 53405 (1.961 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.8874\n",
      "INFO:tensorflow:loss = 1.03629, step = 53505 (2.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.1844\n",
      "INFO:tensorflow:loss = 0.534427, step = 53605 (2.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.4218\n",
      "INFO:tensorflow:loss = 1.25494, step = 53705 (2.067 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.6805\n",
      "INFO:tensorflow:loss = 0.668134, step = 53805 (1.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.606\n",
      "INFO:tensorflow:loss = 1.13142, step = 53905 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7233\n",
      "INFO:tensorflow:loss = 0.56392, step = 54005 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6601\n",
      "INFO:tensorflow:loss = 1.24715, step = 54105 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6302\n",
      "INFO:tensorflow:loss = 1.29278, step = 54205 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7274\n",
      "INFO:tensorflow:loss = 0.93631, step = 54305 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5725\n",
      "INFO:tensorflow:loss = 0.808281, step = 54405 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6626\n",
      "INFO:tensorflow:loss = 1.07133, step = 54505 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6204\n",
      "INFO:tensorflow:loss = 1.0343, step = 54605 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6335\n",
      "INFO:tensorflow:loss = 0.93215, step = 54705 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6526\n",
      "INFO:tensorflow:loss = 1.39518, step = 54805 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4828\n",
      "INFO:tensorflow:loss = 1.01001, step = 54905 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4912\n",
      "INFO:tensorflow:loss = 1.32531, step = 55005 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6663\n",
      "INFO:tensorflow:loss = 0.711896, step = 55105 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.788\n",
      "INFO:tensorflow:loss = 1.02184, step = 55205 (1.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7556\n",
      "INFO:tensorflow:loss = 1.21919, step = 55305 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5182\n",
      "INFO:tensorflow:loss = 1.05422, step = 55405 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6685\n",
      "INFO:tensorflow:loss = 0.780379, step = 55505 (1.898 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 52.728\n",
      "INFO:tensorflow:loss = 0.846412, step = 55605 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5774\n",
      "INFO:tensorflow:loss = 0.822508, step = 55705 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6754\n",
      "INFO:tensorflow:loss = 1.46466, step = 55805 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7091\n",
      "INFO:tensorflow:loss = 0.835922, step = 55905 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5408\n",
      "INFO:tensorflow:loss = 0.983007, step = 56005 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6039\n",
      "INFO:tensorflow:loss = 0.714362, step = 56105 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7437\n",
      "INFO:tensorflow:loss = 0.776842, step = 56205 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7229\n",
      "INFO:tensorflow:loss = 0.802132, step = 56305 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6813\n",
      "INFO:tensorflow:loss = 0.833566, step = 56405 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6503\n",
      "INFO:tensorflow:loss = 0.804038, step = 56505 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7848\n",
      "INFO:tensorflow:loss = 0.892646, step = 56605 (1.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7142\n",
      "INFO:tensorflow:loss = 1.10258, step = 56705 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7843\n",
      "INFO:tensorflow:loss = 0.872571, step = 56805 (1.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.762\n",
      "INFO:tensorflow:loss = 0.926708, step = 56905 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7186\n",
      "INFO:tensorflow:loss = 0.488074, step = 57005 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7367\n",
      "INFO:tensorflow:loss = 0.874166, step = 57105 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.8081\n",
      "INFO:tensorflow:loss = 0.740499, step = 57205 (1.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5673\n",
      "INFO:tensorflow:loss = 1.20202, step = 57305 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7206\n",
      "INFO:tensorflow:loss = 0.910129, step = 57405 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.586\n",
      "INFO:tensorflow:loss = 0.985108, step = 57505 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6368\n",
      "INFO:tensorflow:loss = 0.857916, step = 57605 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6925\n",
      "INFO:tensorflow:loss = 0.692332, step = 57705 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5224\n",
      "INFO:tensorflow:loss = 1.00819, step = 57805 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7717\n",
      "INFO:tensorflow:loss = 0.909507, step = 57905 (1.895 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 58004 into /tmp/image_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.25436.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-21-00:39:17\n",
      "INFO:tensorflow:Restoring parameters from /tmp/image_convnet_model/model.ckpt-58004\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-21-00:39:17\n",
      "INFO:tensorflow:Saving dict for global step 58004: accuracy = 0.6543, global_step = 58004, loss = 0.925879\n"
     ]
    }
   ],
   "source": [
    "augmented_results = run_experiment(train_data_AUGM, train_labels_AUGM, eval_data, eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASIC RESULTS:  {'accuracy': 0.19432998, 'loss': 3.5603511, 'global_step': 10000}\n"
     ]
    }
   ],
   "source": [
    "print(\"BASIC RESULTS: \", basic_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUGMENTED RESULTS:  {'accuracy': 0.65430015, 'loss': 0.9258793, 'global_step': 58004}\n"
     ]
    }
   ],
   "source": [
    "print(\"AUGMENTED RESULTS: \", augmented_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Accuracy of all classes\n",
      "0.716075599594\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "# n_estimators is the number of decision trees\n",
    "# max_features also known as m_try is set to the default value of the square root of the number of features\n",
    "clf = RF(n_estimators=100, n_jobs=3);\n",
    "scores = cross_validation.cross_val_score(clf, X, y, cv=5, n_jobs=1);\n",
    "print(\"Accuracy of all classes\")\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                               precision    recall  f1-score   support\n",
      "\n",
      "                  competition_data/train_small/detritus_other       0.63      0.90      0.75       914\n",
      "              competition_data/train_small/hydromedusae_typeF       0.80      0.61      0.69        61\n",
      "competition_data/train_small/ctenophore_cydippid_no_tentacles       0.00      0.00      0.00        42\n",
      "      competition_data/train_small/copepod_calanoid_eucalanus       0.97      0.38      0.54        96\n",
      "            competition_data/train_small/detritus_filamentous       0.54      0.46      0.50       394\n",
      "              competition_data/train_small/hydromedusae_typeE       0.00      0.00      0.00        14\n",
      "               competition_data/train_small/ctenophore_cestid       1.00      0.08      0.15       113\n",
      "                competition_data/train_small/crustacean_other       0.75      0.63      0.68       201\n",
      "        competition_data/train_small/appendicularian_straight       0.69      0.29      0.40       242\n",
      "              competition_data/train_small/acantharia_protist       0.87      0.94      0.90       889\n",
      "\n",
      "                                                  avg / total       0.72      0.72      0.68      2966\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(y, n_folds=5)\n",
    "y_pred = y * 0\n",
    "for train, test in kf:\n",
    "    X_train, X_test, y_train, y_test = X[train,:], X[test,:], y[train], y[test]\n",
    "    clf = RF(n_estimators=100, n_jobs=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred[test] = clf.predict(X_test)\n",
    "print(classification_report(y, y_pred, target_names=namesClasses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current model, while somewhat accurate overall, doesn't do well for all classes, including the shrimp caridean, stomatopod, or hydromedusae tentacles classes. For others it does quite well, getting many of the correct classifications for trichodesmium_puff and copepod_oithona_eggs classes. The metrics shown above for measuring model performance include precision, recall, and f1-score. The precision metric gives probability that a chosen class is correct, (true positives / (true positive + false positives)), while recall measures the ability of the model correctly classify examples of a given class, (true positives / (false negatives + true positives)). The F1 score is the geometric average of the precision and recall.\n",
    "\n",
    "The competition scoring uses a multiclass log-loss metric to compute your overall score. In the next steps, we define the multiclass log-loss function and compute your estimated score on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiclass_log_loss(y_true, y_pred, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    https://www.kaggle.com/wiki/MultiClassLogLoss\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array, shape = [n_samples]\n",
    "            true class, intergers in [0, n_classes - 1)\n",
    "    y_pred : array, shape = [n_samples, n_classes]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "    \"\"\"\n",
    "    predictions = np.clip(y_pred, eps, 1 - eps)\n",
    "\n",
    "    # normalize row sums to 1\n",
    "    predictions /= predictions.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    actual = np.zeros(y_pred.shape)\n",
    "    n_samples = actual.shape[0]\n",
    "    actual[np.arange(n_samples), y_true.astype(int)] = 1\n",
    "    vectsum = np.sum(actual * np.log(predictions))\n",
    "    loss = -1.0 / n_samples * vectsum\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the probability predictions for computing the log-loss function\n",
    "kf = KFold(y, n_folds=5)\n",
    "# prediction probabilities number of samples, by number of classes\n",
    "y_pred = np.zeros((len(y),len(set(y))))\n",
    "for train, test in kf:\n",
    "    X_train, X_test, y_train, y_test = X[train,:], X[test,:], y[train], y[test]\n",
    "    clf = RF(n_estimators=100, n_jobs=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred[test] = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96844833753168413"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_log_loss(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2966,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Accuracy of all classes\n",
      "0.742152463304\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "# n_estimators is the number of decision trees\n",
    "# max_features also known as m_try is set to the default value of the square root of the number of features\n",
    "clf = RF(n_estimators=100, n_jobs=3);\n",
    "scores = cross_validation.cross_val_score(clf, X_aug, y_aug, cv=5, n_jobs=1);\n",
    "print(\"Accuracy of all classes\")\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the probability predictions for computing the log-loss function\n",
    "kf = KFold(y_aug, n_folds=5)\n",
    "# prediction probabilities number of samples, by number of classes\n",
    "y_aug_pred = np.zeros((len(y_aug),len(set(y_aug))))\n",
    "for train, test in kf:\n",
    "    X_train, X_test, y_train, y_test = X_aug[train,:], X_aug[test,:], y_aug[train], y_aug[test]\n",
    "    clf = RF(n_estimators=100, n_jobs=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_aug_pred[test] = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83528301329128241"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_log_loss(y_aug, y_aug_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mix type of y not allowed, got types {'continuous-multioutput', 'multiclass'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-e6bee7de603e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_aug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_aug_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnamesClasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/honours/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/honours/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mix type of y not allowed, got types %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mys_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mlabel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mys_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mix type of y not allowed, got types {'continuous-multioutput', 'multiclass'}"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_aug, y_aug_pred, target_names=namesClasses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142368, 10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_aug_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiclass log loss function is an classification error metric that heavily penalizes you for being both confident (either predicting very high or very low class probability) and wrong. Throughout the competition you will want to check that your model improvements are driving this loss metric lower.\n",
    "\n",
    "#### Where to Go From Here\n",
    "\n",
    "Now that you've made a simple metric, created a model, and examined the model's performance on the training data, the next step is to make improvements to your model to make it more competitive. The random forest model we created does not perform evenly across all classes and in some cases fails completely. By creating new features and looking at some of your distributions for the problem classes directly, you can identify features that specifically help separate those classes from the others. You can add new metrics by considering other image properties, stratified sampling, transformations, or other models for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (2373, 626) (593, 626) (2373,) (593,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len(set(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

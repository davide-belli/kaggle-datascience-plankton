{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davide/miniconda3/envs/honours/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Import libraries for doing image analysis\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "import glob\n",
    "import os\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import StratifiedKFold as KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from pylab import cm\n",
    "from skimage import segmentation\n",
    "from skimage.morphology import watershed\n",
    "from skimage import measure\n",
    "from skimage import morphology\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import ndimage\n",
    "from skimage.feature import peak_local_max\n",
    "import tensorflow as tf\n",
    "from math import ceil, floor\n",
    "\n",
    "# make graphics inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the classnames from the directory structure\n",
    "directory_names = list(set(glob.glob(os.path.join(\"competition_data\",\"train_small\", \"*\"))\\\n",
    " ).difference(set(glob.glob(os.path.join(\"competition_data\",\"train_small\",\"*.*\")))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading images\n",
      "10.0 % done\n",
      "20.0 % done\n",
      "30.0 % done\n",
      "40.0 % done\n",
      "50.0 % done\n",
      "60.0 % done\n",
      "70.0 % done\n",
      "80.0 % done\n",
      "90.0 % done\n",
      "100.0 % done\n"
     ]
    }
   ],
   "source": [
    "# Rescale the images and create the combined metrics and training labels\n",
    "\n",
    "#get the total training images\n",
    "numberofImages = 0\n",
    "for folder in directory_names:\n",
    "    for fileNameDir in os.walk(folder):   \n",
    "        for fileName in fileNameDir[2]:\n",
    "             # Only read in the images\n",
    "            if fileName[-4:] != \".jpg\":\n",
    "              continue\n",
    "            numberofImages += 1\n",
    "            \n",
    "# We'll rescale the images to be 25x25\n",
    "maxPixel = 28\n",
    "imageSize = maxPixel * maxPixel\n",
    "num_rows = numberofImages # one row for each image in the training dataset\n",
    "num_features = imageSize # + 1 # for our ratio\n",
    "\n",
    "# X is the feature vector with one row of features per image\n",
    "# consisting of the pixel values and our metric\n",
    "X = np.zeros((num_rows, num_features), dtype=float)\n",
    "# y is the numeric class label \n",
    "y = np.zeros((num_rows))\n",
    "\n",
    "files = []\n",
    "# Generate training data\n",
    "i = 0    \n",
    "label = 0\n",
    "# List of string of class names\n",
    "namesClasses = list()\n",
    "\n",
    "print(\"Reading images\")\n",
    "# Navigate through the list of directories\n",
    "for folder in directory_names:\n",
    "    # Append the string class name for each class\n",
    "    currentClass = folder.split(os.pathsep)[-1]\n",
    "    namesClasses.append(currentClass)\n",
    "    for fileNameDir in os.walk(folder):   \n",
    "        for fileName in fileNameDir[2]:\n",
    "            # Only read in the images\n",
    "            if fileName[-4:] != \".jpg\":\n",
    "              continue\n",
    "\n",
    "            # Read in the images and create the features\n",
    "            nameFileImage = \"{0}{1}{2}\".format(fileNameDir[0], os.sep, fileName)            \n",
    "            image = imread(nameFileImage, as_grey=True)\n",
    "            files.append(nameFileImage)\n",
    "            #axisratio = getMinorMajorRatio(image)\n",
    "            image = resize(image, (maxPixel, maxPixel))\n",
    "\n",
    "            # Store the rescaled image pixels and the axis ratio\n",
    "            X[i, 0:imageSize] = np.reshape(image, (1, imageSize))\n",
    "            #X[i, imageSize] = axisratio\n",
    "\n",
    "            # Store the classlabel\n",
    "            y[i] = label\n",
    "            i += 1\n",
    "            # report progress for each 5% done  \n",
    "            report = [int((j+1)*num_rows/10.) for j in range(10)]\n",
    "            if i in report: \n",
    "                print (np.ceil(i *100.0 / num_rows), \"% done\")\n",
    "    label += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = maxPixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flip_images(X_imgs):\n",
    "    X_flip = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape = (IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "    tf_img1 = tf.image.flip_left_right(X)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for img in X_imgs:\n",
    "            flipped_imgs = sess.run([tf_img1], feed_dict = {X: img})\n",
    "            X_flip.extend(flipped_imgs)\n",
    "    X_flip = np.array(X_flip, dtype = np.float32)\n",
    "    return np.concatenate((X_imgs, X_flip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rotate_images(X_imgs):\n",
    "    X_rotate = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape = (IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "    k = tf.placeholder(tf.int32)\n",
    "    tf_img = tf.image.rot90(X, k = k)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for img in X_imgs:\n",
    "            for i in range(3):  # Rotation at 90, 180 and 270 degrees\n",
    "                rotated_img = sess.run(tf_img, feed_dict = {X: img, k: i + 1})\n",
    "                X_rotate.append(rotated_img)\n",
    "        \n",
    "    X_rotate = np.array(X_rotate, dtype = np.float32)\n",
    "    return np.concatenate((X_imgs, X_rotate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_images(X_imgs, scales=[0.80, 0.90]):\n",
    "    # Various settings needed for Tensorflow operation\n",
    "    boxes = np.zeros((len(scales), 4), dtype = np.float32)\n",
    "    for index, scale in enumerate(scales):\n",
    "        x1 = y1 = 0.5 - 0.5 * scale # To scale centrally\n",
    "        x2 = y2 = 0.5 + 0.5 * scale\n",
    "        boxes[index] = np.array([y1, x1, y2, x2], dtype = np.float32)\n",
    "    box_ind = np.zeros((len(scales)), dtype = np.int32)\n",
    "    crop_size = np.array([maxPixel, maxPixel], dtype = np.int32)\n",
    "    \n",
    "    X_scale_data = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape = (1, maxPixel, maxPixel, 1))\n",
    "    # Define Tensorflow operation for all scales but only one base image at a time\n",
    "    tf_img = tf.image.crop_and_resize(X, boxes, box_ind, crop_size)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for img_data in X_imgs:\n",
    "            batch_img = np.expand_dims(img_data, axis = 0)\n",
    "            scaled_imgs = sess.run(tf_img, feed_dict = {X: batch_img})\n",
    "            X_scale_data.extend(scaled_imgs)\n",
    "    \n",
    "    X_scale_data = np.array(X_scale_data, dtype = np.float32)\n",
    "    return np.concatenate((X_imgs, X_scale_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_translate_parameters(index):\n",
    "    if index == 0: # Translate left 20 percent\n",
    "        offset = np.array([0.0, 0.2], dtype = np.float32)\n",
    "        size = np.array([IMAGE_SIZE, ceil(0.8 * IMAGE_SIZE)], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = int(ceil(0.8 * IMAGE_SIZE))\n",
    "        h_start = 0\n",
    "        h_end = IMAGE_SIZE\n",
    "    elif index == 1: # Translate right 20 percent\n",
    "        offset = np.array([0.0, -0.2], dtype = np.float32)\n",
    "        size = np.array([IMAGE_SIZE, ceil(0.8 * IMAGE_SIZE)], dtype = np.int32)\n",
    "        w_start = int(floor((1 - 0.8) * IMAGE_SIZE))\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = 0\n",
    "        h_end = IMAGE_SIZE\n",
    "    elif index == 2: # Translate top 20 percent\n",
    "        offset = np.array([0.2, 0.0], dtype = np.float32)\n",
    "        size = np.array([ceil(0.8 * IMAGE_SIZE), IMAGE_SIZE], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = 0\n",
    "        h_end = int(ceil(0.8 * IMAGE_SIZE)) \n",
    "    else: # Translate bottom 20 percent\n",
    "        offset = np.array([-0.2, 0.0], dtype = np.float32)\n",
    "        size = np.array([ceil(0.8 * IMAGE_SIZE), IMAGE_SIZE], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = int(floor((1 - 0.8) * IMAGE_SIZE))\n",
    "        h_end = IMAGE_SIZE \n",
    "        \n",
    "    return offset, size, w_start, w_end, h_start, h_end\n",
    "\n",
    "def translate_images(X_imgs):\n",
    "    offsets = np.zeros((len(X_imgs), 2), dtype = np.float32)\n",
    "    n_translations = 4\n",
    "    X_translated_arr = []\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(n_translations):\n",
    "            X_translated = np.zeros((len(X_imgs), IMAGE_SIZE, IMAGE_SIZE, 1), \n",
    "\t\t\t\t    dtype = np.float32)\n",
    "            X_translated.fill(1.0) # Filling background color\n",
    "            base_offset, size, w_start, w_end, h_start, h_end = get_translate_parameters(i)\n",
    "            offsets[:, :] = base_offset \n",
    "            glimpses = tf.image.extract_glimpse(X_imgs, size, offsets)\n",
    "            \n",
    "            glimpses = sess.run(glimpses)\n",
    "            X_translated[:, h_start: h_start + size[0], \\\n",
    "\t\t\t w_start: w_start + size[1], :] = glimpses\n",
    "            X_translated_arr.extend(X_translated)\n",
    "    X_translated_arr = np.array(X_translated_arr, dtype = np.float32)\n",
    "    return np.concatenate((X_imgs, X_translated_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment_dataset(X_imgs, y_imgs):\n",
    "    assert len(X_imgs) == len(y_imgs)\n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "    i = 0\n",
    "    print(\"Starting Dataset Augmentation...\")\n",
    "    \n",
    "    for i in range(len(y_imgs)):\n",
    "        \n",
    "        imgs = [np.reshape(X_imgs[i,:],(IMAGE_SIZE,IMAGE_SIZE,1))]\n",
    "        label = [y_imgs[i]]\n",
    "        \n",
    "        imgs = flip_images(imgs)\n",
    "        imgs = rotate_images(imgs)\n",
    "        imgs = scale_images(imgs)\n",
    "        imgs = translate_images(imgs)\n",
    "        \n",
    "        labels = label * len(imgs)\n",
    "#         print(len(labels), imgs.shape)\n",
    "        \n",
    "        X_aug += [np.array(x) for x in imgs.tolist()[:]]\n",
    "        y_aug += labels\n",
    "        \n",
    "        i += 1\n",
    "        # report progress for each 5% done  \n",
    "        report = [int((j+1)*len(y_imgs)/10.) for j in range(10)]\n",
    "        if i in report: \n",
    "            print (np.ceil(i *100.0 / len(y_imgs)), \"% done\")\n",
    "        \n",
    "    return X_aug, y_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "N_CLASSES = len(set(y))\n",
    "print(N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "    # Plankton images are 25x25 pixels, and have one color channel\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "    # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2\n",
    "    # Computes 64 features using a 5x5 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "    # Dense Layer\n",
    "    # Densely connected layer with 1024 neurons\n",
    "    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    # Output Tensor Shape: [batch_size, 1024]\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "    dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits layer\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    logits = tf.layers.dense(inputs=dropout, units=N_CLASSES)\n",
    "\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train / Total:  2373 / 2966\n",
      "Shapes: (2373, 784) (2373,) (593, 784) (593,)\n"
     ]
    }
   ],
   "source": [
    "#Generate indexes to shuffle dataset and create testset\n",
    "\n",
    "idx = np.arange(len(y))\n",
    "np.random.seed(seed=1234)\n",
    "np.random.shuffle(idx)\n",
    "separator = ceil(len(y)/10*8)\n",
    "print(\"Train / Total: \", separator, \"/\", len(y))\n",
    "\n",
    "train_data = X[idx[:separator]].astype('float32')\n",
    "eval_data = X[idx[separator:]].astype('float32')\n",
    "train_labels = y[idx[:separator]].astype(int)\n",
    "eval_labels = y[idx[separator:]].astype(int)\n",
    "\n",
    "train_data = train_data #[:, 0:-1] # Returns np.array, remove axis rateo feature\n",
    "eval_data = eval_data #[:, 0:-1]\n",
    "\n",
    "print(\"Shapes:\", train_data.shape, train_labels.shape, eval_data.shape, eval_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_experiment(train_data, train_labels, eval_data, eval_labels):\n",
    "    # Create the Estimator\n",
    "    image_classifier = tf.estimator.Estimator(\n",
    "      model_fn=cnn_model_fn, model_dir=\"/tmp/image_convnet_model\")\n",
    "\n",
    "    # Set up logging for predictions\n",
    "    # Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
    "    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "    # Train the model\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": train_data},\n",
    "      y=train_labels,\n",
    "      batch_size=100,\n",
    "      num_epochs=None,\n",
    "      shuffle=True)\n",
    "    image_classifier.train(\n",
    "      input_fn=train_input_fn,\n",
    "      steps=10000)#,\n",
    "      #hooks=[logging_hook])\n",
    "\n",
    "    # Evaluate the model and print results\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": eval_data},\n",
    "      y=eval_labels,\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "    eval_results = image_classifier.evaluate(input_fn=eval_input_fn)\n",
    "#     print(eval_results)\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/image_convnet_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd299748278>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/image_convnet_model/model.ckpt-38004\n",
      "INFO:tensorflow:Saving checkpoints for 38005 into /tmp/image_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 5.57383, step = 38005\n",
      "INFO:tensorflow:global_step/sec: 51.3406\n",
      "INFO:tensorflow:loss = 1.48757, step = 38105 (1.949 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5149\n",
      "INFO:tensorflow:loss = 1.17099, step = 38205 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4675\n",
      "INFO:tensorflow:loss = 1.34276, step = 38305 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6235\n",
      "INFO:tensorflow:loss = 1.52783, step = 38405 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.1531\n",
      "INFO:tensorflow:loss = 1.2122, step = 38505 (2.035 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3744\n",
      "INFO:tensorflow:loss = 1.31034, step = 38605 (1.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4878\n",
      "INFO:tensorflow:loss = 1.07199, step = 38705 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4899\n",
      "INFO:tensorflow:loss = 1.37102, step = 38805 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.4574\n",
      "INFO:tensorflow:loss = 1.22446, step = 38905 (1.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.6807\n",
      "INFO:tensorflow:loss = 1.05051, step = 39005 (1.935 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.69\n",
      "INFO:tensorflow:loss = 1.08571, step = 39105 (1.935 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.8163\n",
      "INFO:tensorflow:loss = 1.02257, step = 39205 (1.930 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.7039\n",
      "INFO:tensorflow:loss = 1.0111, step = 39305 (1.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.5353\n",
      "INFO:tensorflow:loss = 1.11999, step = 39405 (1.978 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.1542\n",
      "INFO:tensorflow:loss = 1.25707, step = 39505 (1.955 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.321\n",
      "INFO:tensorflow:loss = 0.960712, step = 39605 (1.948 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.0242\n",
      "INFO:tensorflow:loss = 1.13263, step = 39705 (1.960 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.8799\n",
      "INFO:tensorflow:loss = 0.830879, step = 39805 (1.928 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.5459\n",
      "INFO:tensorflow:loss = 1.04358, step = 39905 (1.940 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.0938\n",
      "INFO:tensorflow:loss = 0.933976, step = 40005 (1.957 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.445\n",
      "INFO:tensorflow:loss = 0.967223, step = 40105 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.3405\n",
      "INFO:tensorflow:loss = 0.951235, step = 40205 (1.948 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.7027\n",
      "INFO:tensorflow:loss = 0.922034, step = 40305 (1.934 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.8962\n",
      "INFO:tensorflow:loss = 0.932035, step = 40405 (1.927 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.6447\n",
      "INFO:tensorflow:loss = 0.994453, step = 40505 (1.936 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.8772\n",
      "INFO:tensorflow:loss = 0.903999, step = 40605 (1.928 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.7262\n",
      "INFO:tensorflow:loss = 0.905335, step = 40705 (1.933 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2259\n",
      "INFO:tensorflow:loss = 0.830475, step = 40805 (1.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2075\n",
      "INFO:tensorflow:loss = 1.0531, step = 40905 (1.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.3698\n",
      "INFO:tensorflow:loss = 0.957805, step = 41005 (1.946 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.9602\n",
      "INFO:tensorflow:loss = 0.882226, step = 41105 (2.042 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.7916\n",
      "INFO:tensorflow:loss = 0.814875, step = 41205 (2.093 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.9261\n",
      "INFO:tensorflow:loss = 0.878303, step = 41305 (2.003 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3754\n",
      "INFO:tensorflow:loss = 1.03388, step = 41405 (1.910 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2268\n",
      "INFO:tensorflow:loss = 0.769325, step = 41505 (1.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3307\n",
      "INFO:tensorflow:loss = 0.799489, step = 41605 (1.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4196\n",
      "INFO:tensorflow:loss = 0.704346, step = 41705 (1.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.5935\n",
      "INFO:tensorflow:loss = 0.905436, step = 41805 (1.938 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4241\n",
      "INFO:tensorflow:loss = 0.847228, step = 41905 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.6094\n",
      "INFO:tensorflow:loss = 1.07854, step = 42005 (1.939 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.0834\n",
      "INFO:tensorflow:loss = 0.816102, step = 42105 (1.956 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.6407\n",
      "INFO:tensorflow:loss = 0.892991, step = 42205 (2.016 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.6899\n",
      "INFO:tensorflow:loss = 0.883528, step = 42305 (2.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.0177\n",
      "INFO:tensorflow:loss = 0.822639, step = 42405 (2.000 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.2448\n",
      "INFO:tensorflow:loss = 0.821513, step = 42505 (2.072 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.3437\n",
      "INFO:tensorflow:loss = 0.844942, step = 42605 (2.069 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.1415\n",
      "INFO:tensorflow:loss = 0.93416, step = 42705 (2.077 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.1508\n",
      "INFO:tensorflow:loss = 0.761393, step = 42805 (2.077 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.5815\n",
      "INFO:tensorflow:loss = 0.771348, step = 42905 (1.939 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.0176\n",
      "INFO:tensorflow:loss = 0.978532, step = 43005 (1.922 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4665\n",
      "INFO:tensorflow:loss = 0.780085, step = 43105 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1912\n",
      "INFO:tensorflow:loss = 0.676937, step = 43205 (1.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.2587\n",
      "INFO:tensorflow:loss = 0.691811, step = 43305 (2.030 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4372\n",
      "INFO:tensorflow:loss = 0.874003, step = 43405 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.424\n",
      "INFO:tensorflow:loss = 0.858003, step = 43505 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1314\n",
      "INFO:tensorflow:loss = 1.02309, step = 43605 (1.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1765\n",
      "INFO:tensorflow:loss = 0.744966, step = 43705 (1.916 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4733\n",
      "INFO:tensorflow:loss = 0.801678, step = 43805 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.218\n",
      "INFO:tensorflow:loss = 0.807716, step = 43905 (2.074 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.8889\n",
      "INFO:tensorflow:loss = 0.656545, step = 44005 (2.132 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.2014\n",
      "INFO:tensorflow:loss = 0.869643, step = 44105 (2.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5337\n",
      "INFO:tensorflow:loss = 0.880167, step = 44205 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.2706\n",
      "INFO:tensorflow:loss = 0.942938, step = 44305 (2.030 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.338\n",
      "INFO:tensorflow:loss = 0.897424, step = 44405 (2.070 sec)\n",
      "INFO:tensorflow:global_step/sec: 44.9367\n",
      "INFO:tensorflow:loss = 0.624509, step = 44505 (2.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.116\n",
      "INFO:tensorflow:loss = 0.764022, step = 44605 (1.994 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.4506\n",
      "INFO:tensorflow:loss = 0.772398, step = 44705 (2.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.1284\n",
      "INFO:tensorflow:loss = 0.857178, step = 44805 (2.036 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.0296\n",
      "INFO:tensorflow:loss = 0.722374, step = 44905 (2.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.3533\n",
      "INFO:tensorflow:loss = 0.857988, step = 45005 (2.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.0083\n",
      "INFO:tensorflow:loss = 0.699232, step = 45105 (2.126 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.7744\n",
      "INFO:tensorflow:loss = 0.778942, step = 45205 (2.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.2627\n",
      "INFO:tensorflow:loss = 0.866589, step = 45305 (2.072 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.782\n",
      "INFO:tensorflow:loss = 0.73603, step = 45405 (2.050 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7868\n",
      "INFO:tensorflow:loss = 0.736995, step = 45505 (1.894 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 52.8881\n",
      "INFO:tensorflow:loss = 0.717944, step = 45605 (1.891 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.0967\n",
      "INFO:tensorflow:loss = 0.702485, step = 45705 (1.919 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.8859\n",
      "INFO:tensorflow:loss = 0.747649, step = 45805 (1.965 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.0951\n",
      "INFO:tensorflow:loss = 0.851448, step = 45905 (1.957 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.8312\n",
      "INFO:tensorflow:loss = 0.829759, step = 46005 (2.007 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.1936\n",
      "INFO:tensorflow:loss = 0.798954, step = 46105 (2.119 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5797\n",
      "INFO:tensorflow:loss = 0.641517, step = 46205 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.1722\n",
      "INFO:tensorflow:loss = 0.714148, step = 46305 (1.952 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.5143\n",
      "INFO:tensorflow:loss = 0.708661, step = 46405 (1.941 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.2767\n",
      "INFO:tensorflow:loss = 0.803285, step = 46505 (1.951 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.0939\n",
      "INFO:tensorflow:loss = 0.792338, step = 46605 (1.957 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.3308\n",
      "INFO:tensorflow:loss = 0.595069, step = 46705 (1.987 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3155\n",
      "INFO:tensorflow:loss = 0.741374, step = 46805 (1.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.4447\n",
      "INFO:tensorflow:loss = 0.859262, step = 46905 (1.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.4592\n",
      "INFO:tensorflow:loss = 0.707603, step = 47005 (1.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.5859\n",
      "INFO:tensorflow:loss = 0.767125, step = 47105 (1.939 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5814\n",
      "INFO:tensorflow:loss = 0.562485, step = 47205 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4178\n",
      "INFO:tensorflow:loss = 0.513799, step = 47305 (1.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.7073\n",
      "INFO:tensorflow:loss = 0.791781, step = 47405 (1.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.6798\n",
      "INFO:tensorflow:loss = 0.651684, step = 47505 (2.014 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.3546\n",
      "INFO:tensorflow:loss = 0.59885, step = 47605 (2.067 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.7962\n",
      "INFO:tensorflow:loss = 0.730782, step = 47705 (2.051 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.5346\n",
      "INFO:tensorflow:loss = 0.661901, step = 47805 (2.059 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.6243\n",
      "INFO:tensorflow:loss = 0.643748, step = 47905 (2.057 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 48004 into /tmp/image_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.730686.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-21-00:21:02\n",
      "INFO:tensorflow:Restoring parameters from /tmp/image_convnet_model/model.ckpt-48004\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-21-00:21:02\n",
      "INFO:tensorflow:Saving dict for global step 48004: accuracy = 0.780776, global_step = 48004, loss = 0.651073\n"
     ]
    }
   ],
   "source": [
    "basic_results = run_experiment(train_data, train_labels, eval_data, eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Dataset Augmentation...\n",
      "10.0 % done\n",
      "20.0 % done\n",
      "30.0 % done\n",
      "40.0 % done\n",
      "50.0 % done\n",
      "60.0 % done\n",
      "70.0 % done\n",
      "80.0 % done\n",
      "90.0 % done\n",
      "100.0 % done\n",
      "Augmented dataset:  (284760, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data_AUGM, train_labels_AUGM = augment_dataset(train_data, train_labels)\n",
    "\n",
    "train_data_AUGM = np.array(train_data_AUGM, dtype='float32')\n",
    "train_labels_AUGM = np.array(train_labels_AUGM, dtype='int')\n",
    "\n",
    "print(\"Augmented dataset: \", train_data_AUGM.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/image_convnet_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8f806f0358>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/image_convnet_model/model.ckpt-48004\n",
      "INFO:tensorflow:Saving checkpoints for 48005 into /tmp/image_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.88094, step = 48005\n",
      "INFO:tensorflow:global_step/sec: 51.4643\n",
      "INFO:tensorflow:loss = 0.751765, step = 48105 (1.944 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5298\n",
      "INFO:tensorflow:loss = 1.66778, step = 48205 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4896\n",
      "INFO:tensorflow:loss = 0.634546, step = 48305 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5702\n",
      "INFO:tensorflow:loss = 1.70944, step = 48405 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6196\n",
      "INFO:tensorflow:loss = 1.62926, step = 48505 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7197\n",
      "INFO:tensorflow:loss = 1.25364, step = 48605 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.402\n",
      "INFO:tensorflow:loss = 1.23313, step = 48705 (1.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6306\n",
      "INFO:tensorflow:loss = 1.18166, step = 48805 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6033\n",
      "INFO:tensorflow:loss = 1.25284, step = 48905 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5694\n",
      "INFO:tensorflow:loss = 1.18679, step = 49005 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4402\n",
      "INFO:tensorflow:loss = 0.909477, step = 49105 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.449\n",
      "INFO:tensorflow:loss = 1.00726, step = 49205 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7426\n",
      "INFO:tensorflow:loss = 1.70312, step = 49305 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7079\n",
      "INFO:tensorflow:loss = 1.15865, step = 49405 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5979\n",
      "INFO:tensorflow:loss = 0.425586, step = 49505 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5552\n",
      "INFO:tensorflow:loss = 1.02166, step = 49605 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5378\n",
      "INFO:tensorflow:loss = 1.06767, step = 49705 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.9161\n",
      "INFO:tensorflow:loss = 1.19798, step = 49805 (2.006 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.8738\n",
      "INFO:tensorflow:loss = 1.15977, step = 49905 (2.086 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.4827\n",
      "INFO:tensorflow:loss = 0.797161, step = 50005 (2.062 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.6888\n",
      "INFO:tensorflow:loss = 1.29992, step = 50105 (1.935 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.4588\n",
      "INFO:tensorflow:loss = 0.787399, step = 50205 (1.982 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.6277\n",
      "INFO:tensorflow:loss = 1.185, step = 50305 (1.975 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5283\n",
      "INFO:tensorflow:loss = 0.842573, step = 50405 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4754\n",
      "INFO:tensorflow:loss = 0.924698, step = 50505 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5738\n",
      "INFO:tensorflow:loss = 1.06958, step = 50605 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5712\n",
      "INFO:tensorflow:loss = 1.32609, step = 50705 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5973\n",
      "INFO:tensorflow:loss = 1.57876, step = 50805 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5995\n",
      "INFO:tensorflow:loss = 1.09915, step = 50905 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6631\n",
      "INFO:tensorflow:loss = 0.609221, step = 51005 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.507\n",
      "INFO:tensorflow:loss = 1.21334, step = 51105 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6444\n",
      "INFO:tensorflow:loss = 0.858901, step = 51205 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4753\n",
      "INFO:tensorflow:loss = 0.889614, step = 51305 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4748\n",
      "INFO:tensorflow:loss = 1.15496, step = 51405 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5153\n",
      "INFO:tensorflow:loss = 0.739447, step = 51505 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5545\n",
      "INFO:tensorflow:loss = 1.00167, step = 51605 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5269\n",
      "INFO:tensorflow:loss = 0.878426, step = 51705 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5226\n",
      "INFO:tensorflow:loss = 1.05458, step = 51805 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4661\n",
      "INFO:tensorflow:loss = 1.18199, step = 51905 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.493\n",
      "INFO:tensorflow:loss = 0.758671, step = 52005 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5031\n",
      "INFO:tensorflow:loss = 1.10006, step = 52105 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5052\n",
      "INFO:tensorflow:loss = 1.6013, step = 52205 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.483\n",
      "INFO:tensorflow:loss = 1.03733, step = 52305 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4569\n",
      "INFO:tensorflow:loss = 1.01774, step = 52405 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6346\n",
      "INFO:tensorflow:loss = 1.39261, step = 52505 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4983\n",
      "INFO:tensorflow:loss = 0.82376, step = 52605 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5319\n",
      "INFO:tensorflow:loss = 0.699305, step = 52705 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5258\n",
      "INFO:tensorflow:loss = 1.20071, step = 52805 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4786\n",
      "INFO:tensorflow:loss = 0.923829, step = 52905 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7608\n",
      "INFO:tensorflow:loss = 0.923757, step = 53005 (1.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.639\n",
      "INFO:tensorflow:loss = 1.04544, step = 53105 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3674\n",
      "INFO:tensorflow:loss = 0.71289, step = 53205 (1.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4874\n",
      "INFO:tensorflow:loss = 1.23527, step = 53305 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.0128\n",
      "INFO:tensorflow:loss = 0.931275, step = 53405 (1.961 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.8874\n",
      "INFO:tensorflow:loss = 1.03629, step = 53505 (2.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.1844\n",
      "INFO:tensorflow:loss = 0.534427, step = 53605 (2.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.4218\n",
      "INFO:tensorflow:loss = 1.25494, step = 53705 (2.067 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.6805\n",
      "INFO:tensorflow:loss = 0.668134, step = 53805 (1.972 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.606\n",
      "INFO:tensorflow:loss = 1.13142, step = 53905 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7233\n",
      "INFO:tensorflow:loss = 0.56392, step = 54005 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6601\n",
      "INFO:tensorflow:loss = 1.24715, step = 54105 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6302\n",
      "INFO:tensorflow:loss = 1.29278, step = 54205 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7274\n",
      "INFO:tensorflow:loss = 0.93631, step = 54305 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5725\n",
      "INFO:tensorflow:loss = 0.808281, step = 54405 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6626\n",
      "INFO:tensorflow:loss = 1.07133, step = 54505 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6204\n",
      "INFO:tensorflow:loss = 1.0343, step = 54605 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6335\n",
      "INFO:tensorflow:loss = 0.93215, step = 54705 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6526\n",
      "INFO:tensorflow:loss = 1.39518, step = 54805 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4828\n",
      "INFO:tensorflow:loss = 1.01001, step = 54905 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4912\n",
      "INFO:tensorflow:loss = 1.32531, step = 55005 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6663\n",
      "INFO:tensorflow:loss = 0.711896, step = 55105 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.788\n",
      "INFO:tensorflow:loss = 1.02184, step = 55205 (1.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7556\n",
      "INFO:tensorflow:loss = 1.21919, step = 55305 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5182\n",
      "INFO:tensorflow:loss = 1.05422, step = 55405 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6685\n",
      "INFO:tensorflow:loss = 0.780379, step = 55505 (1.898 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 52.728\n",
      "INFO:tensorflow:loss = 0.846412, step = 55605 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5774\n",
      "INFO:tensorflow:loss = 0.822508, step = 55705 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6754\n",
      "INFO:tensorflow:loss = 1.46466, step = 55805 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7091\n",
      "INFO:tensorflow:loss = 0.835922, step = 55905 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5408\n",
      "INFO:tensorflow:loss = 0.983007, step = 56005 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6039\n",
      "INFO:tensorflow:loss = 0.714362, step = 56105 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7437\n",
      "INFO:tensorflow:loss = 0.776842, step = 56205 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7229\n",
      "INFO:tensorflow:loss = 0.802132, step = 56305 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6813\n",
      "INFO:tensorflow:loss = 0.833566, step = 56405 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6503\n",
      "INFO:tensorflow:loss = 0.804038, step = 56505 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7848\n",
      "INFO:tensorflow:loss = 0.892646, step = 56605 (1.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7142\n",
      "INFO:tensorflow:loss = 1.10258, step = 56705 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7843\n",
      "INFO:tensorflow:loss = 0.872571, step = 56805 (1.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.762\n",
      "INFO:tensorflow:loss = 0.926708, step = 56905 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7186\n",
      "INFO:tensorflow:loss = 0.488074, step = 57005 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7367\n",
      "INFO:tensorflow:loss = 0.874166, step = 57105 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.8081\n",
      "INFO:tensorflow:loss = 0.740499, step = 57205 (1.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5673\n",
      "INFO:tensorflow:loss = 1.20202, step = 57305 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7206\n",
      "INFO:tensorflow:loss = 0.910129, step = 57405 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.586\n",
      "INFO:tensorflow:loss = 0.985108, step = 57505 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6368\n",
      "INFO:tensorflow:loss = 0.857916, step = 57605 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6925\n",
      "INFO:tensorflow:loss = 0.692332, step = 57705 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5224\n",
      "INFO:tensorflow:loss = 1.00819, step = 57805 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7717\n",
      "INFO:tensorflow:loss = 0.909507, step = 57905 (1.895 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 58004 into /tmp/image_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.25436.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-21-00:39:17\n",
      "INFO:tensorflow:Restoring parameters from /tmp/image_convnet_model/model.ckpt-58004\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-21-00:39:17\n",
      "INFO:tensorflow:Saving dict for global step 58004: accuracy = 0.6543, global_step = 58004, loss = 0.925879\n"
     ]
    }
   ],
   "source": [
    "augmented_results = run_experiment(train_data_AUGM, train_labels_AUGM, eval_data, eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASIC RESULTS:  {'accuracy': 0.78077573, 'loss': 0.6510731, 'global_step': 48004}\n"
     ]
    }
   ],
   "source": [
    "print(\"BASIC RESULTS: \", basic_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUGMENTED RESULTS:  {'accuracy': 0.65430015, 'loss': 0.9258793, 'global_step': 58004}\n"
     ]
    }
   ],
   "source": [
    "print(\"AUGMENTED RESULTS: \", augmented_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Accuracy of all classes\n",
      "0.716075599594\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "# n_estimators is the number of decision trees\n",
    "# max_features also known as m_try is set to the default value of the square root of the number of features\n",
    "clf = RF(n_estimators=100, n_jobs=3);\n",
    "scores = cross_validation.cross_val_score(clf, X, y, cv=5, n_jobs=1);\n",
    "print(\"Accuracy of all classes\")\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                               precision    recall  f1-score   support\n",
      "\n",
      "                  competition_data/train_small/detritus_other       0.63      0.90      0.75       914\n",
      "              competition_data/train_small/hydromedusae_typeF       0.80      0.61      0.69        61\n",
      "competition_data/train_small/ctenophore_cydippid_no_tentacles       0.00      0.00      0.00        42\n",
      "      competition_data/train_small/copepod_calanoid_eucalanus       0.97      0.38      0.54        96\n",
      "            competition_data/train_small/detritus_filamentous       0.54      0.46      0.50       394\n",
      "              competition_data/train_small/hydromedusae_typeE       0.00      0.00      0.00        14\n",
      "               competition_data/train_small/ctenophore_cestid       1.00      0.08      0.15       113\n",
      "                competition_data/train_small/crustacean_other       0.75      0.63      0.68       201\n",
      "        competition_data/train_small/appendicularian_straight       0.69      0.29      0.40       242\n",
      "              competition_data/train_small/acantharia_protist       0.87      0.94      0.90       889\n",
      "\n",
      "                                                  avg / total       0.72      0.72      0.68      2966\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(y, n_folds=5)\n",
    "y_pred = y * 0\n",
    "for train, test in kf:\n",
    "    X_train, X_test, y_train, y_test = X[train,:], X[test,:], y[train], y[test]\n",
    "    clf = RF(n_estimators=100, n_jobs=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred[test] = clf.predict(X_test)\n",
    "print(classification_report(y, y_pred, target_names=namesClasses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current model, while somewhat accurate overall, doesn't do well for all classes, including the shrimp caridean, stomatopod, or hydromedusae tentacles classes. For others it does quite well, getting many of the correct classifications for trichodesmium_puff and copepod_oithona_eggs classes. The metrics shown above for measuring model performance include precision, recall, and f1-score. The precision metric gives probability that a chosen class is correct, (true positives / (true positive + false positives)), while recall measures the ability of the model correctly classify examples of a given class, (true positives / (false negatives + true positives)). The F1 score is the geometric average of the precision and recall.\n",
    "\n",
    "The competition scoring uses a multiclass log-loss metric to compute your overall score. In the next steps, we define the multiclass log-loss function and compute your estimated score on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiclass_log_loss(y_true, y_pred, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    https://www.kaggle.com/wiki/MultiClassLogLoss\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array, shape = [n_samples]\n",
    "            true class, intergers in [0, n_classes - 1)\n",
    "    y_pred : array, shape = [n_samples, n_classes]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "    \"\"\"\n",
    "    predictions = np.clip(y_pred, eps, 1 - eps)\n",
    "\n",
    "    # normalize row sums to 1\n",
    "    predictions /= predictions.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    actual = np.zeros(y_pred.shape)\n",
    "    n_samples = actual.shape[0]\n",
    "    actual[np.arange(n_samples), y_true.astype(int)] = 1\n",
    "    vectsum = np.sum(actual * np.log(predictions))\n",
    "    loss = -1.0 / n_samples * vectsum\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the probability predictions for computing the log-loss function\n",
    "kf = KFold(y, n_folds=5)\n",
    "# prediction probabilities number of samples, by number of classes\n",
    "y_pred = np.zeros((len(y),len(set(y))))\n",
    "for train, test in kf:\n",
    "    X_train, X_test, y_train, y_test = X[train,:], X[test,:], y[train], y[test]\n",
    "    clf = RF(n_estimators=100, n_jobs=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred[test] = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96844833753168413"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_log_loss(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2966,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Accuracy of all classes\n",
      "0.742152463304\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "# n_estimators is the number of decision trees\n",
    "# max_features also known as m_try is set to the default value of the square root of the number of features\n",
    "clf = RF(n_estimators=100, n_jobs=3);\n",
    "scores = cross_validation.cross_val_score(clf, X_aug, y_aug, cv=5, n_jobs=1);\n",
    "print(\"Accuracy of all classes\")\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the probability predictions for computing the log-loss function\n",
    "kf = KFold(y_aug, n_folds=5)\n",
    "# prediction probabilities number of samples, by number of classes\n",
    "y_aug_pred = np.zeros((len(y_aug),len(set(y_aug))))\n",
    "for train, test in kf:\n",
    "    X_train, X_test, y_train, y_test = X_aug[train,:], X_aug[test,:], y_aug[train], y_aug[test]\n",
    "    clf = RF(n_estimators=100, n_jobs=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_aug_pred[test] = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83528301329128241"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_log_loss(y_aug, y_aug_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mix type of y not allowed, got types {'continuous-multioutput', 'multiclass'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-e6bee7de603e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_aug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_aug_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnamesClasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/honours/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/honours/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mix type of y not allowed, got types %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mys_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mlabel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mys_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mix type of y not allowed, got types {'continuous-multioutput', 'multiclass'}"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_aug, y_aug_pred, target_names=namesClasses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142368, 10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_aug_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiclass log loss function is an classification error metric that heavily penalizes you for being both confident (either predicting very high or very low class probability) and wrong. Throughout the competition you will want to check that your model improvements are driving this loss metric lower.\n",
    "\n",
    "#### Where to Go From Here\n",
    "\n",
    "Now that you've made a simple metric, created a model, and examined the model's performance on the training data, the next step is to make improvements to your model to make it more competitive. The random forest model we created does not perform evenly across all classes and in some cases fails completely. By creating new features and looking at some of your distributions for the problem classes directly, you can identify features that specifically help separate those classes from the others. You can add new metrics by considering other image properties, stratified sampling, transformations, or other models for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (2373, 626) (593, 626) (2373,) (593,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len(set(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

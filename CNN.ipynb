{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will go step-by-step through a simple model to distinguish different types of plankton and demonstrate some tools for exploring the image dataset. We will start by going through an example of one image to show how you could choose to develop a metric based on the shape of the object within the image. First, we import the necessary modules from scikit-image, matplotlib, scikit-learn, and numpy. If you don't currently have python installed, you can get the Anaconda distribution that includes all of the referenced packages below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davide/miniconda3/envs/honours/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Import libraries for doing image analysis\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "import glob\n",
    "import os\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import StratifiedKFold as KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from pylab import cm\n",
    "from skimage import segmentation\n",
    "from skimage.morphology import watershed\n",
    "from skimage import measure\n",
    "from skimage import morphology\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import ndimage\n",
    "from skimage.feature import peak_local_max\n",
    "import tensorflow as tf\n",
    "from math import ceil, floor\n",
    "\n",
    "# make graphics inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Data\n",
    "\n",
    "The training data is organized in a series of subdirectories that contain examples for the each class of interest. We will store the list of directory names to aid in labelling the data classes for training and testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the classnames from the directory structure\n",
    "directory_names = list(set(glob.glob(os.path.join(\"competition_data\",\"train_small\", \"*\"))\\\n",
    " ).difference(set(glob.glob(os.path.join(\"competition_data\",\"train_small\",\"*.*\")))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading images\n",
      "5.0 % done\n",
      "10.0 % done\n",
      "15.0 % done\n",
      "20.0 % done\n",
      "25.0 % done\n",
      "30.0 % done\n",
      "35.0 % done\n",
      "40.0 % done\n",
      "45.0 % done\n",
      "50.0 % done\n",
      "55.0 % done\n",
      "60.0 % done\n",
      "65.0 % done\n",
      "70.0 % done\n",
      "75.0 % done\n",
      "80.0 % done\n",
      "85.0 % done\n",
      "90.0 % done\n",
      "95.0 % done\n",
      "100.0 % done\n"
     ]
    }
   ],
   "source": [
    "# Rescale the images and create the combined metrics and training labels\n",
    "\n",
    "#get the total training images\n",
    "numberofImages = 0\n",
    "for folder in directory_names:\n",
    "    for fileNameDir in os.walk(folder):   \n",
    "        for fileName in fileNameDir[2]:\n",
    "             # Only read in the images\n",
    "            if fileName[-4:] != \".jpg\":\n",
    "              continue\n",
    "            numberofImages += 1\n",
    "            \n",
    "# We'll rescale the images to be 25x25\n",
    "maxPixel = 28\n",
    "imageSize = maxPixel * maxPixel\n",
    "num_rows = numberofImages # one row for each image in the training dataset\n",
    "num_features = imageSize # + 1 # for our ratio\n",
    "\n",
    "# X is the feature vector with one row of features per image\n",
    "# consisting of the pixel values and our metric\n",
    "X = np.zeros((num_rows, num_features), dtype=float)\n",
    "# y is the numeric class label \n",
    "y = np.zeros((num_rows))\n",
    "\n",
    "files = []\n",
    "# Generate training data\n",
    "i = 0    \n",
    "label = 0\n",
    "# List of string of class names\n",
    "namesClasses = list()\n",
    "\n",
    "print(\"Reading images\")\n",
    "# Navigate through the list of directories\n",
    "for folder in directory_names:\n",
    "    # Append the string class name for each class\n",
    "    currentClass = folder.split(os.pathsep)[-1]\n",
    "    namesClasses.append(currentClass)\n",
    "    for fileNameDir in os.walk(folder):   \n",
    "        for fileName in fileNameDir[2]:\n",
    "            # Only read in the images\n",
    "            if fileName[-4:] != \".jpg\":\n",
    "              continue\n",
    "\n",
    "            # Read in the images and create the features\n",
    "            nameFileImage = \"{0}{1}{2}\".format(fileNameDir[0], os.sep, fileName)            \n",
    "            image = imread(nameFileImage, as_grey=True)\n",
    "            files.append(nameFileImage)\n",
    "            #axisratio = getMinorMajorRatio(image)\n",
    "            image = resize(image, (maxPixel, maxPixel))\n",
    "\n",
    "            # Store the rescaled image pixels and the axis ratio\n",
    "            X[i, 0:imageSize] = np.reshape(image, (1, imageSize))\n",
    "            #X[i, imageSize] = axisratio\n",
    "\n",
    "            # Store the classlabel\n",
    "            y[i] = label\n",
    "            i += 1\n",
    "            # report progress for each 5% done  \n",
    "            report = [int((j+1)*num_rows/20.) for j in range(20)]\n",
    "            if i in report: \n",
    "                print (np.ceil(i *100.0 / num_rows), \"% done\")\n",
    "    label += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = maxPixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flip_images(X_imgs):\n",
    "    X_flip = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape = (IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "    tf_img1 = tf.image.flip_left_right(X)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for img in X_imgs:\n",
    "            flipped_imgs = sess.run([tf_img1], feed_dict = {X: img})\n",
    "            X_flip.extend(flipped_imgs)\n",
    "    X_flip = np.array(X_flip, dtype = np.float32)\n",
    "    return np.concatenate((X_imgs, X_flip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rotate_images(X_imgs):\n",
    "    X_rotate = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape = (IMAGE_SIZE, IMAGE_SIZE, 1))\n",
    "    k = tf.placeholder(tf.int32)\n",
    "    tf_img = tf.image.rot90(X, k = k)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for img in X_imgs:\n",
    "            for i in range(3):  # Rotation at 90, 180 and 270 degrees\n",
    "                rotated_img = sess.run(tf_img, feed_dict = {X: img, k: i + 1})\n",
    "                X_rotate.append(rotated_img)\n",
    "        \n",
    "    X_rotate = np.array(X_rotate, dtype = np.float32)\n",
    "    return np.concatenate((X_imgs, X_rotate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_images(X_imgs, scales=[0.80, 0.90]):\n",
    "    # Various settings needed for Tensorflow operation\n",
    "    boxes = np.zeros((len(scales), 4), dtype = np.float32)\n",
    "    for index, scale in enumerate(scales):\n",
    "        x1 = y1 = 0.5 - 0.5 * scale # To scale centrally\n",
    "        x2 = y2 = 0.5 + 0.5 * scale\n",
    "        boxes[index] = np.array([y1, x1, y2, x2], dtype = np.float32)\n",
    "    box_ind = np.zeros((len(scales)), dtype = np.int32)\n",
    "    crop_size = np.array([maxPixel, maxPixel], dtype = np.int32)\n",
    "    \n",
    "    X_scale_data = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape = (1, maxPixel, maxPixel, 1))\n",
    "    # Define Tensorflow operation for all scales but only one base image at a time\n",
    "    tf_img = tf.image.crop_and_resize(X, boxes, box_ind, crop_size)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for img_data in X_imgs:\n",
    "            batch_img = np.expand_dims(img_data, axis = 0)\n",
    "            scaled_imgs = sess.run(tf_img, feed_dict = {X: batch_img})\n",
    "            X_scale_data.extend(scaled_imgs)\n",
    "    \n",
    "    X_scale_data = np.array(X_scale_data, dtype = np.float32)\n",
    "    return np.concatenate((X_imgs, X_scale_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_translate_parameters(index):\n",
    "    if index == 0: # Translate left 20 percent\n",
    "        offset = np.array([0.0, 0.2], dtype = np.float32)\n",
    "        size = np.array([IMAGE_SIZE, ceil(0.8 * IMAGE_SIZE)], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = int(ceil(0.8 * IMAGE_SIZE))\n",
    "        h_start = 0\n",
    "        h_end = IMAGE_SIZE\n",
    "    elif index == 1: # Translate right 20 percent\n",
    "        offset = np.array([0.0, -0.2], dtype = np.float32)\n",
    "        size = np.array([IMAGE_SIZE, ceil(0.8 * IMAGE_SIZE)], dtype = np.int32)\n",
    "        w_start = int(floor((1 - 0.8) * IMAGE_SIZE))\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = 0\n",
    "        h_end = IMAGE_SIZE\n",
    "    elif index == 2: # Translate top 20 percent\n",
    "        offset = np.array([0.2, 0.0], dtype = np.float32)\n",
    "        size = np.array([ceil(0.8 * IMAGE_SIZE), IMAGE_SIZE], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = 0\n",
    "        h_end = int(ceil(0.8 * IMAGE_SIZE)) \n",
    "    else: # Translate bottom 20 percent\n",
    "        offset = np.array([-0.2, 0.0], dtype = np.float32)\n",
    "        size = np.array([ceil(0.8 * IMAGE_SIZE), IMAGE_SIZE], dtype = np.int32)\n",
    "        w_start = 0\n",
    "        w_end = IMAGE_SIZE\n",
    "        h_start = int(floor((1 - 0.8) * IMAGE_SIZE))\n",
    "        h_end = IMAGE_SIZE \n",
    "        \n",
    "    return offset, size, w_start, w_end, h_start, h_end\n",
    "\n",
    "def translate_images(X_imgs):\n",
    "    offsets = np.zeros((len(X_imgs), 2), dtype = np.float32)\n",
    "    n_translations = 4\n",
    "    X_translated_arr = []\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(n_translations):\n",
    "            X_translated = np.zeros((len(X_imgs), IMAGE_SIZE, IMAGE_SIZE, 1), \n",
    "\t\t\t\t    dtype = np.float32)\n",
    "            X_translated.fill(1.0) # Filling background color\n",
    "            base_offset, size, w_start, w_end, h_start, h_end = get_translate_parameters(i)\n",
    "            offsets[:, :] = base_offset \n",
    "            glimpses = tf.image.extract_glimpse(X_imgs, size, offsets)\n",
    "            \n",
    "            glimpses = sess.run(glimpses)\n",
    "            X_translated[:, h_start: h_start + size[0], \\\n",
    "\t\t\t w_start: w_start + size[1], :] = glimpses\n",
    "            X_translated_arr.extend(X_translated)\n",
    "    X_translated_arr = np.array(X_translated_arr, dtype = np.float32)\n",
    "    return np.concatenate((X_imgs, X_translated_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment_dataset(X_imgs, y_imgs):\n",
    "    assert len(X_imgs) == len(y_imgs)\n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "    i = 0\n",
    "    \n",
    "    for i in range(len(y_imgs)):\n",
    "        \n",
    "        imgs = [np.reshape(X_imgs[i,:],(IMAGE_SIZE,IMAGE_SIZE,1))]\n",
    "        label = [y_imgs[i]]\n",
    "        \n",
    "        imgs = flip_images(imgs)\n",
    "        imgs = rotate_images(imgs)\n",
    "        imgs = scale_images(imgs)\n",
    "        imgs = translate_images(imgs)\n",
    "        \n",
    "        labels = label * len(imgs)\n",
    "#         print(len(labels), imgs.shape)\n",
    "        \n",
    "        X_aug += [np.array(x) for x in imgs.tolist()[:]]\n",
    "        y_aug += labels\n",
    "        \n",
    "        i += 1\n",
    "        # report progress for each 5% done  \n",
    "        report = [int((j+1)*num_rows/20.) for j in range(20)]\n",
    "        if i in report: \n",
    "            print (np.ceil(i *100.0 / num_rows), \"% done\")\n",
    "        \n",
    "    return X_aug, y_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_aug, y_aug = augment_dataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_aug = np.reshape(np.array(X_aug), (len(X_aug),imageSize))\n",
    "# y_aug = np.array(y_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "N_CLASSES = len(set(y))\n",
    "print(N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "    # Plankton images are 25x25 pixels, and have one color channel\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "    # Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2\n",
    "    # Computes 64 features using a 5x5 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "    # Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=64,\n",
    "      kernel_size=[5, 5],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "    # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "    # Dense Layer\n",
    "    # Densely connected layer with 1024 neurons\n",
    "    # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "    # Output Tensor Shape: [batch_size, 1024]\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "    dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits layer\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    logits = tf.layers.dense(inputs=dropout, units=N_CLASSES)\n",
    "\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train / Total:  2373 / 2966\n",
      "Shapes: (2373, 784) (2373,) (593, 784) (593,)\n"
     ]
    }
   ],
   "source": [
    "#Generate indexes to shuffle dataset and create testset\n",
    "\n",
    "idx = np.arange(len(y))\n",
    "np.random.seed(seed=1234)\n",
    "np.random.shuffle(idx)\n",
    "separator = ceil(len(y)/10*8)\n",
    "print(\"Train / Total: \", separator, \"/\", len(y))\n",
    "\n",
    "train_data = X[idx[:separator]].astype('float32')\n",
    "eval_data = X[idx[separator:]].astype('float32')\n",
    "train_labels = y[idx[:separator]].astype(int)\n",
    "eval_labels = y[idx[separator:]].astype(int)\n",
    "\n",
    "train_data = train_data #[:, 0:-1] # Returns np.array, remove axis rateo feature\n",
    "eval_data = eval_data #[:, 0:-1]\n",
    "\n",
    "print(\"Shapes:\", train_data.shape, train_labels.shape, eval_data.shape, eval_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(train_data, train_labels, eval_data, eval_labels):\n",
    "    # Create the Estimator\n",
    "    image_classifier = tf.estimator.Estimator(\n",
    "      model_fn=cnn_model_fn, model_dir=\"/tmp/image_convnet_model\")\n",
    "\n",
    "    # Set up logging for predictions\n",
    "    # Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
    "    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "    # Train the model\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": train_data},\n",
    "      y=train_labels,\n",
    "      batch_size=100,\n",
    "      num_epochs=None,\n",
    "      shuffle=True)\n",
    "    image_classifier.train(\n",
    "      input_fn=train_input_fn,\n",
    "      steps=10000)#,\n",
    "      #hooks=[logging_hook])\n",
    "\n",
    "    # Evaluate the model and print results\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": eval_data},\n",
    "      y=eval_labels,\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "    eval_results = image_classifier.evaluate(input_fn=eval_input_fn)\n",
    "#     print(eval_results)\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/image_convnet_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff9b8b7d1d0>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/image_convnet_model/model.ckpt-18004\n",
      "INFO:tensorflow:Saving checkpoints for 18005 into /tmp/image_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.04895, step = 18005\n",
      "INFO:tensorflow:global_step/sec: 51.669\n",
      "INFO:tensorflow:loss = 0.98933, step = 18105 (1.936 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.281\n",
      "INFO:tensorflow:loss = 0.982313, step = 18205 (1.913 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6177\n",
      "INFO:tensorflow:loss = 0.931104, step = 18305 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5374\n",
      "INFO:tensorflow:loss = 0.977606, step = 18405 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5265\n",
      "INFO:tensorflow:loss = 1.03321, step = 18505 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4493\n",
      "INFO:tensorflow:loss = 1.01053, step = 18605 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4472\n",
      "INFO:tensorflow:loss = 1.01425, step = 18705 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4921\n",
      "INFO:tensorflow:loss = 1.03268, step = 18805 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5265\n",
      "INFO:tensorflow:loss = 0.912536, step = 18905 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.9922\n",
      "INFO:tensorflow:loss = 0.870298, step = 19005 (2.041 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.007\n",
      "INFO:tensorflow:loss = 0.942074, step = 19105 (2.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 50.1075\n",
      "INFO:tensorflow:loss = 0.96091, step = 19205 (1.996 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3808\n",
      "INFO:tensorflow:loss = 1.01611, step = 19305 (1.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5772\n",
      "INFO:tensorflow:loss = 0.925816, step = 19405 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4405\n",
      "INFO:tensorflow:loss = 0.904742, step = 19505 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.9808\n",
      "INFO:tensorflow:loss = 0.866529, step = 19605 (1.924 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5171\n",
      "INFO:tensorflow:loss = 0.831971, step = 19705 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.6818\n",
      "INFO:tensorflow:loss = 1.04641, step = 19805 (2.014 sec)\n",
      "INFO:tensorflow:global_step/sec: 48.6084\n",
      "INFO:tensorflow:loss = 0.94818, step = 19905 (2.056 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4755\n",
      "INFO:tensorflow:loss = 0.869763, step = 20005 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4262\n",
      "INFO:tensorflow:loss = 0.911873, step = 20105 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4974\n",
      "INFO:tensorflow:loss = 0.935281, step = 20205 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4419\n",
      "INFO:tensorflow:loss = 0.898871, step = 20305 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6729\n",
      "INFO:tensorflow:loss = 0.851084, step = 20405 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4417\n",
      "INFO:tensorflow:loss = 0.855827, step = 20505 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5472\n",
      "INFO:tensorflow:loss = 0.939241, step = 20605 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3414\n",
      "INFO:tensorflow:loss = 0.818042, step = 20705 (1.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3893\n",
      "INFO:tensorflow:loss = 0.903115, step = 20805 (1.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5689\n",
      "INFO:tensorflow:loss = 0.921118, step = 20905 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3693\n",
      "INFO:tensorflow:loss = 0.904329, step = 21005 (1.910 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7797\n",
      "INFO:tensorflow:loss = 0.876881, step = 21105 (1.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5151\n",
      "INFO:tensorflow:loss = 1.1094, step = 21205 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4247\n",
      "INFO:tensorflow:loss = 0.736656, step = 21305 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6047\n",
      "INFO:tensorflow:loss = 0.828919, step = 21405 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7394\n",
      "INFO:tensorflow:loss = 0.737789, step = 21505 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5045\n",
      "INFO:tensorflow:loss = 0.923124, step = 21605 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5107\n",
      "INFO:tensorflow:loss = 0.849842, step = 21705 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5113\n",
      "INFO:tensorflow:loss = 0.894868, step = 21805 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.547\n",
      "INFO:tensorflow:loss = 0.717227, step = 21905 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4706\n",
      "INFO:tensorflow:loss = 0.935911, step = 22005 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3938\n",
      "INFO:tensorflow:loss = 0.76185, step = 22105 (1.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5891\n",
      "INFO:tensorflow:loss = 0.985853, step = 22205 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6217\n",
      "INFO:tensorflow:loss = 0.803351, step = 22305 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4692\n",
      "INFO:tensorflow:loss = 0.948044, step = 22405 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4029\n",
      "INFO:tensorflow:loss = 0.739308, step = 22505 (1.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4177\n",
      "INFO:tensorflow:loss = 0.91286, step = 22605 (1.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6595\n",
      "INFO:tensorflow:loss = 0.910757, step = 22705 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3589\n",
      "INFO:tensorflow:loss = 0.748734, step = 22805 (1.910 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4512\n",
      "INFO:tensorflow:loss = 0.780012, step = 22905 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.65\n",
      "INFO:tensorflow:loss = 0.641418, step = 23005 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6807\n",
      "INFO:tensorflow:loss = 0.776315, step = 23105 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.9407\n",
      "INFO:tensorflow:loss = 0.774478, step = 23205 (1.889 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.619\n",
      "INFO:tensorflow:loss = 0.808007, step = 23305 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5276\n",
      "INFO:tensorflow:loss = 0.76022, step = 23405 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.652\n",
      "INFO:tensorflow:loss = 0.958392, step = 23505 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6777\n",
      "INFO:tensorflow:loss = 0.848527, step = 23605 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.533\n",
      "INFO:tensorflow:loss = 0.849899, step = 23705 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5275\n",
      "INFO:tensorflow:loss = 0.81605, step = 23805 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7176\n",
      "INFO:tensorflow:loss = 0.825341, step = 23905 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5007\n",
      "INFO:tensorflow:loss = 0.930385, step = 24005 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6959\n",
      "INFO:tensorflow:loss = 0.84363, step = 24105 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7021\n",
      "INFO:tensorflow:loss = 0.894469, step = 24205 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6978\n",
      "INFO:tensorflow:loss = 0.932726, step = 24305 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5936\n",
      "INFO:tensorflow:loss = 0.867033, step = 24405 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5706\n",
      "INFO:tensorflow:loss = 0.798395, step = 24505 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7491\n",
      "INFO:tensorflow:loss = 0.597902, step = 24605 (1.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5167\n",
      "INFO:tensorflow:loss = 0.774302, step = 24705 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6307\n",
      "INFO:tensorflow:loss = 0.79436, step = 24805 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5672\n",
      "INFO:tensorflow:loss = 0.809074, step = 24905 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5677\n",
      "INFO:tensorflow:loss = 0.821563, step = 25005 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6601\n",
      "INFO:tensorflow:loss = 0.765687, step = 25105 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7577\n",
      "INFO:tensorflow:loss = 0.796027, step = 25205 (1.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7014\n",
      "INFO:tensorflow:loss = 0.648992, step = 25305 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6001\n",
      "INFO:tensorflow:loss = 0.735633, step = 25405 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4059\n",
      "INFO:tensorflow:loss = 0.96507, step = 25505 (1.908 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 52.8797\n",
      "INFO:tensorflow:loss = 0.68804, step = 25605 (1.891 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.9474\n",
      "INFO:tensorflow:loss = 0.746939, step = 25705 (1.889 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4234\n",
      "INFO:tensorflow:loss = 0.699626, step = 25805 (1.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7663\n",
      "INFO:tensorflow:loss = 0.863017, step = 25905 (1.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4104\n",
      "INFO:tensorflow:loss = 0.726468, step = 26005 (1.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6606\n",
      "INFO:tensorflow:loss = 0.657925, step = 26105 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7378\n",
      "INFO:tensorflow:loss = 0.766014, step = 26205 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.1248\n",
      "INFO:tensorflow:loss = 0.609184, step = 26305 (1.919 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4089\n",
      "INFO:tensorflow:loss = 0.799613, step = 26405 (1.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5334\n",
      "INFO:tensorflow:loss = 0.873273, step = 26505 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.525\n",
      "INFO:tensorflow:loss = 0.685561, step = 26605 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6752\n",
      "INFO:tensorflow:loss = 0.741422, step = 26705 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.8079\n",
      "INFO:tensorflow:loss = 0.832735, step = 26805 (1.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5737\n",
      "INFO:tensorflow:loss = 0.676044, step = 26905 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5206\n",
      "INFO:tensorflow:loss = 0.602816, step = 27005 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6267\n",
      "INFO:tensorflow:loss = 0.729126, step = 27105 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6644\n",
      "INFO:tensorflow:loss = 0.806819, step = 27205 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5589\n",
      "INFO:tensorflow:loss = 0.825069, step = 27305 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5472\n",
      "INFO:tensorflow:loss = 0.673327, step = 27405 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5957\n",
      "INFO:tensorflow:loss = 0.722413, step = 27505 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7198\n",
      "INFO:tensorflow:loss = 0.703865, step = 27605 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6726\n",
      "INFO:tensorflow:loss = 0.553652, step = 27705 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7415\n",
      "INFO:tensorflow:loss = 0.692894, step = 27805 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7183\n",
      "INFO:tensorflow:loss = 0.624739, step = 27905 (1.897 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 28004 into /tmp/image_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.713394.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-20-23:58:15\n",
      "INFO:tensorflow:Restoring parameters from /tmp/image_convnet_model/model.ckpt-28004\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-20-23:58:15\n",
      "INFO:tensorflow:Saving dict for global step 28004: accuracy = 0.733558, global_step = 28004, loss = 0.783959\n"
     ]
    }
   ],
   "source": [
    "basic_results = run_experiment(train_data, train_labels, eval_data, eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0 % done\n",
      "10.0 % done\n",
      "15.0 % done\n",
      "20.0 % done\n",
      "25.0 % done\n",
      "30.0 % done\n",
      "35.0 % done\n",
      "40.0 % done\n",
      "45.0 % done\n",
      "50.0 % done\n",
      "55.0 % done\n",
      "60.0 % done\n",
      "65.0 % done\n",
      "70.0 % done\n",
      "75.0 % done\n",
      "80.0 % done\n"
     ]
    }
   ],
   "source": [
    "train_data_AUGM, train_labels_AUGM = augment_dataset(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_AUGM = np.array(train_data_AUGM, dtype='float32')\n",
    "train_labels_AUGM = np.array(train_labels_AUGM, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113904, 28, 28, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_AUGM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/image_convnet_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ffa93527dd8>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/image_convnet_model/model.ckpt-28004\n",
      "INFO:tensorflow:Saving checkpoints for 28005 into /tmp/image_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.866242, step = 28005\n",
      "INFO:tensorflow:global_step/sec: 51.5884\n",
      "INFO:tensorflow:loss = 0.749071, step = 28105 (1.939 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4407\n",
      "INFO:tensorflow:loss = 1.52382, step = 28205 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5018\n",
      "INFO:tensorflow:loss = 0.961508, step = 28305 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7032\n",
      "INFO:tensorflow:loss = 1.41625, step = 28405 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6029\n",
      "INFO:tensorflow:loss = 0.897089, step = 28505 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6751\n",
      "INFO:tensorflow:loss = 1.26191, step = 28605 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5225\n",
      "INFO:tensorflow:loss = 1.08462, step = 28705 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2993\n",
      "INFO:tensorflow:loss = 1.03816, step = 28805 (1.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4956\n",
      "INFO:tensorflow:loss = 1.36169, step = 28905 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5104\n",
      "INFO:tensorflow:loss = 0.78012, step = 29005 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5294\n",
      "INFO:tensorflow:loss = 1.33688, step = 29105 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.529\n",
      "INFO:tensorflow:loss = 0.994787, step = 29205 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4443\n",
      "INFO:tensorflow:loss = 0.648022, step = 29305 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4943\n",
      "INFO:tensorflow:loss = 1.03964, step = 29405 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5918\n",
      "INFO:tensorflow:loss = 1.19819, step = 29505 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3619\n",
      "INFO:tensorflow:loss = 0.960268, step = 29605 (1.910 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4811\n",
      "INFO:tensorflow:loss = 1.05833, step = 29705 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6602\n",
      "INFO:tensorflow:loss = 0.723695, step = 29805 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3798\n",
      "INFO:tensorflow:loss = 0.737938, step = 29905 (1.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5697\n",
      "INFO:tensorflow:loss = 1.16539, step = 30005 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5654\n",
      "INFO:tensorflow:loss = 1.46411, step = 30105 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4255\n",
      "INFO:tensorflow:loss = 0.751654, step = 30205 (1.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4777\n",
      "INFO:tensorflow:loss = 0.882563, step = 30305 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4688\n",
      "INFO:tensorflow:loss = 0.983527, step = 30405 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4787\n",
      "INFO:tensorflow:loss = 0.824, step = 30505 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5359\n",
      "INFO:tensorflow:loss = 1.04925, step = 30605 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6221\n",
      "INFO:tensorflow:loss = 0.949823, step = 30705 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5333\n",
      "INFO:tensorflow:loss = 0.77729, step = 30805 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6077\n",
      "INFO:tensorflow:loss = 1.05929, step = 30905 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3299\n",
      "INFO:tensorflow:loss = 0.929414, step = 31005 (1.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5399\n",
      "INFO:tensorflow:loss = 1.23687, step = 31105 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6282\n",
      "INFO:tensorflow:loss = 0.905278, step = 31205 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6921\n",
      "INFO:tensorflow:loss = 1.10148, step = 31305 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3535\n",
      "INFO:tensorflow:loss = 0.925812, step = 31405 (1.910 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2927\n",
      "INFO:tensorflow:loss = 0.993586, step = 31505 (1.913 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6069\n",
      "INFO:tensorflow:loss = 0.706164, step = 31605 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2121\n",
      "INFO:tensorflow:loss = 0.854748, step = 31705 (1.915 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2522\n",
      "INFO:tensorflow:loss = 1.20236, step = 31805 (1.914 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6517\n",
      "INFO:tensorflow:loss = 0.940534, step = 31905 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5024\n",
      "INFO:tensorflow:loss = 0.603382, step = 32005 (1.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4198\n",
      "INFO:tensorflow:loss = 1.03214, step = 32105 (1.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5152\n",
      "INFO:tensorflow:loss = 1.11914, step = 32205 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7142\n",
      "INFO:tensorflow:loss = 0.919276, step = 32305 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5373\n",
      "INFO:tensorflow:loss = 1.19528, step = 32405 (1.903 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.529\n",
      "INFO:tensorflow:loss = 0.972217, step = 32505 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6947\n",
      "INFO:tensorflow:loss = 1.03285, step = 32605 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5027\n",
      "INFO:tensorflow:loss = 0.708905, step = 32705 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4577\n",
      "INFO:tensorflow:loss = 1.04879, step = 32805 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3462\n",
      "INFO:tensorflow:loss = 0.738895, step = 32905 (1.910 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.2663\n",
      "INFO:tensorflow:loss = 0.854784, step = 33005 (2.030 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5095\n",
      "INFO:tensorflow:loss = 0.906661, step = 33105 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5746\n",
      "INFO:tensorflow:loss = 0.849197, step = 33205 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 49.6618\n",
      "INFO:tensorflow:loss = 0.925795, step = 33305 (2.014 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4345\n",
      "INFO:tensorflow:loss = 1.0287, step = 33405 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6216\n",
      "INFO:tensorflow:loss = 1.285, step = 33505 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6547\n",
      "INFO:tensorflow:loss = 1.00702, step = 33605 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7397\n",
      "INFO:tensorflow:loss = 0.664596, step = 33705 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7465\n",
      "INFO:tensorflow:loss = 0.828754, step = 33805 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7514\n",
      "INFO:tensorflow:loss = 1.08181, step = 33905 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6319\n",
      "INFO:tensorflow:loss = 0.699412, step = 34005 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6493\n",
      "INFO:tensorflow:loss = 1.21538, step = 34105 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.709\n",
      "INFO:tensorflow:loss = 0.99698, step = 34205 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.449\n",
      "INFO:tensorflow:loss = 0.932215, step = 34305 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7628\n",
      "INFO:tensorflow:loss = 1.10726, step = 34405 (1.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5844\n",
      "INFO:tensorflow:loss = 0.960647, step = 34505 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7159\n",
      "INFO:tensorflow:loss = 1.08634, step = 34605 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6705\n",
      "INFO:tensorflow:loss = 0.879139, step = 34705 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6782\n",
      "INFO:tensorflow:loss = 1.0251, step = 34805 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7882\n",
      "INFO:tensorflow:loss = 0.704219, step = 34905 (1.894 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5682\n",
      "INFO:tensorflow:loss = 0.764142, step = 35005 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.529\n",
      "INFO:tensorflow:loss = 0.851417, step = 35105 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6539\n",
      "INFO:tensorflow:loss = 0.89599, step = 35205 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6297\n",
      "INFO:tensorflow:loss = 0.673937, step = 35305 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5946\n",
      "INFO:tensorflow:loss = 0.627345, step = 35405 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4888\n",
      "INFO:tensorflow:loss = 0.698997, step = 35505 (1.905 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 52.5931\n",
      "INFO:tensorflow:loss = 0.598958, step = 35605 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5805\n",
      "INFO:tensorflow:loss = 0.983882, step = 35705 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.539\n",
      "INFO:tensorflow:loss = 1.21298, step = 35805 (1.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6232\n",
      "INFO:tensorflow:loss = 0.722177, step = 35905 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.567\n",
      "INFO:tensorflow:loss = 0.736823, step = 36005 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6842\n",
      "INFO:tensorflow:loss = 0.806179, step = 36105 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5846\n",
      "INFO:tensorflow:loss = 0.720394, step = 36205 (1.902 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6674\n",
      "INFO:tensorflow:loss = 0.765246, step = 36305 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6163\n",
      "INFO:tensorflow:loss = 0.819428, step = 36405 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6368\n",
      "INFO:tensorflow:loss = 0.875851, step = 36505 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7159\n",
      "INFO:tensorflow:loss = 0.982941, step = 36605 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5988\n",
      "INFO:tensorflow:loss = 0.658571, step = 36705 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6314\n",
      "INFO:tensorflow:loss = 0.892648, step = 36805 (1.900 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4665\n",
      "INFO:tensorflow:loss = 0.877635, step = 36905 (1.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7318\n",
      "INFO:tensorflow:loss = 0.734065, step = 37005 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.5633\n",
      "INFO:tensorflow:loss = 0.8282, step = 37105 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7801\n",
      "INFO:tensorflow:loss = 0.736641, step = 37205 (1.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6902\n",
      "INFO:tensorflow:loss = 0.975336, step = 37305 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7592\n",
      "INFO:tensorflow:loss = 0.686904, step = 37405 (1.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7558\n",
      "INFO:tensorflow:loss = 0.876881, step = 37505 (1.896 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6743\n",
      "INFO:tensorflow:loss = 1.09983, step = 37605 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.7061\n",
      "INFO:tensorflow:loss = 0.714113, step = 37705 (1.897 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6603\n",
      "INFO:tensorflow:loss = 0.744124, step = 37805 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6136\n",
      "INFO:tensorflow:loss = 0.941381, step = 37905 (1.901 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 38004 into /tmp/image_convnet_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.03611.\n",
      "INFO:tensorflow:Starting evaluation at 2018-02-21-00:09:43\n",
      "INFO:tensorflow:Restoring parameters from /tmp/image_convnet_model/model.ckpt-38004\n",
      "INFO:tensorflow:Finished evaluation at 2018-02-21-00:09:43\n",
      "INFO:tensorflow:Saving dict for global step 38004: accuracy = 0.703204, global_step = 38004, loss = 0.85365\n"
     ]
    }
   ],
   "source": [
    "augmented_results = run_experiment(train_data_AUGM, train_labels_AUGM, eval_data, eval_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASIC RESULTS:  {'accuracy': 0.73355818, 'loss': 0.78395879, 'global_step': 28004}\n",
      "AUGMENTED RESULTS:  {'accuracy': 0.70320404, 'loss': 0.85364974, 'global_step': 38004}\n"
     ]
    }
   ],
   "source": [
    "print(\"BASIC RESULTS: \", basic_results)\n",
    "print(\"AUGMENTED RESULTS: \", augmented_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Width-to-Length Ratio Class Separation\n",
    "\n",
    "Now that we have calculated the width-to-length ratio metric for all the images, we can look at the class separation to see how well our feature performs. We'll compare pairs of the classes' distributions by plotting each pair of classes. While this will not cover the whole space of hundreds of possible combinations, it will give us a feel for how similar or dissimilar different classes are in this feature, and the class distributions should be comparable across subplots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABq8AAAIfCAYAAADuckZqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f04960dd2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop through the classes two at a time and compare their distributions of the Width/Length Ratio\n",
    "\n",
    "#Create a DataFrame object to make subsetting the data on the class \n",
    "df = pd.DataFrame({\"class\": y[:], \"ratio\": X[:, num_features-1]})\n",
    "\n",
    "f = plt.figure(figsize=(30, 20))\n",
    "#we suppress zeros and choose a few large classes to better highlight the distributions.\n",
    "df = df.loc[df[\"ratio\"] > 0]\n",
    "minimumSize = 20 \n",
    "counts = df[\"class\"].value_counts()\n",
    "largeclasses = [int(x) for x in list(counts.loc[counts > minimumSize].index)]\n",
    "# Loop through 40 of the classes \n",
    "for j in range(0,8,2):\n",
    "    subfig = plt.subplot(2, 4, j/2 +1)\n",
    "    # Plot the normalized histograms for two classes\n",
    "    classind1 = largeclasses[j]\n",
    "    classind2 = largeclasses[j+1]\n",
    "    n, bins,p = plt.hist(df.loc[df[\"class\"] == classind1][\"ratio\"].values,\\\n",
    "                         alpha=0.5, bins=[x*0.01 for x in range(100)], \\\n",
    "                         label=namesClasses[classind1].split(os.sep)[-1], normed=1)\n",
    "\n",
    "    n2, bins,p = plt.hist(df.loc[df[\"class\"] == (classind2)][\"ratio\"].values,\\\n",
    "                          alpha=0.5, bins=bins, label=namesClasses[classind2].split(os.sep)[-1],normed=1)\n",
    "    subfig.set_ylim([0.,10.])\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"Width/Length Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots with Augmented Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABq8AAAIfCAYAAADuckZqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f052078c828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop through the classes two at a time and compare their distributions of the Width/Length Ratio\n",
    "\n",
    "#Create a DataFrame object to make subsetting the data on the class \n",
    "df = pd.DataFrame({\"class\": y_aug[:], \"ratio\": X_aug[:, num_features-1]})\n",
    "\n",
    "f = plt.figure(figsize=(30, 20))\n",
    "#we suppress zeros and choose a few large classes to better highlight the distributions.\n",
    "df = df.loc[df[\"ratio\"] > 0]\n",
    "minimumSize = 20 \n",
    "counts = df[\"class\"].value_counts()\n",
    "largeclasses = [int(x) for x in list(counts.loc[counts > minimumSize].index)]\n",
    "# Loop through 40 of the classes \n",
    "for j in range(0,8,2):\n",
    "    subfig = plt.subplot(2, 4, j/2 +1)\n",
    "    # Plot the normalized histograms for two classes\n",
    "    classind1 = largeclasses[j]\n",
    "    classind2 = largeclasses[j+1]\n",
    "    n, bins,p = plt.hist(df.loc[df[\"class\"] == classind1][\"ratio\"].values,\\\n",
    "                         alpha=0.5, bins=[x*0.01 for x in range(100)], \\\n",
    "                         label=namesClasses[classind1].split(os.sep)[-1], normed=1)\n",
    "\n",
    "    n2, bins,p = plt.hist(df.loc[df[\"class\"] == (classind2)][\"ratio\"].values,\\\n",
    "                          alpha=0.5, bins=bins, label=namesClasses[classind2].split(os.sep)[-1],normed=1)\n",
    "    subfig.set_ylim([0.,10.])\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"Width/Length Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the (truncated) figure above, you will see some cases where the classes are well separated and others were they are not. It is typical that one single feature will not allow you to completely separate more than thirty distinct classes. You will need to be creative in coming up with additional metrics to discriminate between all the classes.\n",
    "\n",
    "#### Random Forest Classification\n",
    "\n",
    "We choose a random forest model to classify the images. Random forests perform well in many classification tasks and have robust default settings. We will give a brief description of a random forest model so that you can understand its two main free parameters: n_estimators and max_features.\n",
    "\n",
    "A random forest model is an ensemble model of n_estimators number of decision trees. During the training process, each decision tree is grown automatically by making a series of conditional splits on the data. At each split in the decision tree, a random sample of max_features number of features is chosen and used to make a conditional decision on which of the two nodes that the data will be grouped in. The best condition for the split is determined by the split that maximizes the class purity of the nodes directly below. The tree continues to grow by making additional splits until the leaves are pure or the leaves have less than the minimum number of samples for a split (in sklearn default for min_samples_split is two data points). The final majority class purity of the terminal nodes of the decision tree are used for making predictions on what class a new data point will belong. Then, the aggregate vote across the forest determines the class prediction for new samples.\n",
    "\n",
    "With our training data consisting of the feature vector X and the class label vector y, we will now calculate some class metrics for the performance of our model, by class and overall. First, we train the random forest on all the available data and let it perform the 5-fold cross validation. Then we perform the cross validation using the KFold method, which splits the data into train and test sets, and a classification report. The classification report provides a useful list of performance metrics for your classifier vs. the internal metrics of the random forest module.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Accuracy of all classes\n",
      "0.716075599594\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "# n_estimators is the number of decision trees\n",
    "# max_features also known as m_try is set to the default value of the square root of the number of features\n",
    "clf = RF(n_estimators=100, n_jobs=3);\n",
    "scores = cross_validation.cross_val_score(clf, X, y, cv=5, n_jobs=1);\n",
    "print(\"Accuracy of all classes\")\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                               precision    recall  f1-score   support\n",
      "\n",
      "                  competition_data/train_small/detritus_other       0.63      0.90      0.75       914\n",
      "              competition_data/train_small/hydromedusae_typeF       0.80      0.61      0.69        61\n",
      "competition_data/train_small/ctenophore_cydippid_no_tentacles       0.00      0.00      0.00        42\n",
      "      competition_data/train_small/copepod_calanoid_eucalanus       0.97      0.38      0.54        96\n",
      "            competition_data/train_small/detritus_filamentous       0.54      0.46      0.50       394\n",
      "              competition_data/train_small/hydromedusae_typeE       0.00      0.00      0.00        14\n",
      "               competition_data/train_small/ctenophore_cestid       1.00      0.08      0.15       113\n",
      "                competition_data/train_small/crustacean_other       0.75      0.63      0.68       201\n",
      "        competition_data/train_small/appendicularian_straight       0.69      0.29      0.40       242\n",
      "              competition_data/train_small/acantharia_protist       0.87      0.94      0.90       889\n",
      "\n",
      "                                                  avg / total       0.72      0.72      0.68      2966\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(y, n_folds=5)\n",
    "y_pred = y * 0\n",
    "for train, test in kf:\n",
    "    X_train, X_test, y_train, y_test = X[train,:], X[test,:], y[train], y[test]\n",
    "    clf = RF(n_estimators=100, n_jobs=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred[test] = clf.predict(X_test)\n",
    "print(classification_report(y, y_pred, target_names=namesClasses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current model, while somewhat accurate overall, doesn't do well for all classes, including the shrimp caridean, stomatopod, or hydromedusae tentacles classes. For others it does quite well, getting many of the correct classifications for trichodesmium_puff and copepod_oithona_eggs classes. The metrics shown above for measuring model performance include precision, recall, and f1-score. The precision metric gives probability that a chosen class is correct, (true positives / (true positive + false positives)), while recall measures the ability of the model correctly classify examples of a given class, (true positives / (false negatives + true positives)). The F1 score is the geometric average of the precision and recall.\n",
    "\n",
    "The competition scoring uses a multiclass log-loss metric to compute your overall score. In the next steps, we define the multiclass log-loss function and compute your estimated score on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiclass_log_loss(y_true, y_pred, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    https://www.kaggle.com/wiki/MultiClassLogLoss\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array, shape = [n_samples]\n",
    "            true class, intergers in [0, n_classes - 1)\n",
    "    y_pred : array, shape = [n_samples, n_classes]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "    \"\"\"\n",
    "    predictions = np.clip(y_pred, eps, 1 - eps)\n",
    "\n",
    "    # normalize row sums to 1\n",
    "    predictions /= predictions.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    actual = np.zeros(y_pred.shape)\n",
    "    n_samples = actual.shape[0]\n",
    "    actual[np.arange(n_samples), y_true.astype(int)] = 1\n",
    "    vectsum = np.sum(actual * np.log(predictions))\n",
    "    loss = -1.0 / n_samples * vectsum\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the probability predictions for computing the log-loss function\n",
    "kf = KFold(y, n_folds=5)\n",
    "# prediction probabilities number of samples, by number of classes\n",
    "y_pred = np.zeros((len(y),len(set(y))))\n",
    "for train, test in kf:\n",
    "    X_train, X_test, y_train, y_test = X[train,:], X[test,:], y[train], y[test]\n",
    "    clf = RF(n_estimators=100, n_jobs=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred[test] = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96844833753168413"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_log_loss(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2966,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Accuracy of all classes\n",
      "0.742152463304\n"
     ]
    }
   ],
   "source": [
    "print(\"Training\")\n",
    "# n_estimators is the number of decision trees\n",
    "# max_features also known as m_try is set to the default value of the square root of the number of features\n",
    "clf = RF(n_estimators=100, n_jobs=3);\n",
    "scores = cross_validation.cross_val_score(clf, X_aug, y_aug, cv=5, n_jobs=1);\n",
    "print(\"Accuracy of all classes\")\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the probability predictions for computing the log-loss function\n",
    "kf = KFold(y_aug, n_folds=5)\n",
    "# prediction probabilities number of samples, by number of classes\n",
    "y_aug_pred = np.zeros((len(y_aug),len(set(y_aug))))\n",
    "for train, test in kf:\n",
    "    X_train, X_test, y_train, y_test = X_aug[train,:], X_aug[test,:], y_aug[train], y_aug[test]\n",
    "    clf = RF(n_estimators=100, n_jobs=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_aug_pred[test] = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83528301329128241"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_log_loss(y_aug, y_aug_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mix type of y not allowed, got types {'continuous-multioutput', 'multiclass'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-e6bee7de603e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_aug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_aug_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnamesClasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/honours/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/honours/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mix type of y not allowed, got types %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mys_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mlabel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mys_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mix type of y not allowed, got types {'continuous-multioutput', 'multiclass'}"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_aug, y_aug_pred, target_names=namesClasses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142368, 10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_aug_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiclass log loss function is an classification error metric that heavily penalizes you for being both confident (either predicting very high or very low class probability) and wrong. Throughout the competition you will want to check that your model improvements are driving this loss metric lower.\n",
    "\n",
    "#### Where to Go From Here\n",
    "\n",
    "Now that you've made a simple metric, created a model, and examined the model's performance on the training data, the next step is to make improvements to your model to make it more competitive. The random forest model we created does not perform evenly across all classes and in some cases fails completely. By creating new features and looking at some of your distributions for the problem classes directly, you can identify features that specifically help separate those classes from the others. You can add new metrics by considering other image properties, stratified sampling, transformations, or other models for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (2373, 626) (593, 626) (2373,) (593,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "593"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len(set(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
